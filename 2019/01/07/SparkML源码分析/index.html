<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="spark," />





  <link rel="alternate" href="/atom.xml" title="Yifan Guo Personal Blog" type="application/atom+xml" />






<meta name="description" content="[TOC] RDD Basic Transformation rdd的操作分为transform和action transform不会触发任务执行，action会    Transformation Meaning     map(func) Return a new distributed dataset formed by passing each element of the source">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkML源码分析">
<meta property="og:url" content="http://www.yifanguo.top/2019/01/07/SparkML源码分析/index.html">
<meta property="og:site_name" content="Yifan Guo Personal Blog">
<meta property="og:description" content="[TOC] RDD Basic Transformation rdd的操作分为transform和action transform不会触发任务执行，action会    Transformation Meaning     map(func) Return a new distributed dataset formed by passing each element of the source">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://www.yifanguo.top/Users/yifanguo/Desktop/blog/source/_posts/SparkML%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/gbdt2.png">
<meta property="og:updated_time" content="2019-01-10T07:24:00.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkML源码分析">
<meta name="twitter:description" content="[TOC] RDD Basic Transformation rdd的操作分为transform和action transform不会触发任务执行，action会    Transformation Meaning     map(func) Return a new distributed dataset formed by passing each element of the source">
<meta name="twitter:image" content="http://www.yifanguo.top/Users/yifanguo/Desktop/blog/source/_posts/SparkML%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/gbdt2.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.yifanguo.top/2019/01/07/SparkML源码分析/"/>





  <title>SparkML源码分析 | Yifan Guo Personal Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <a href="https://github.com/esail" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>


    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yifan Guo Personal Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-http">
          <a href="/http/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            http
          </a>
        </li>
      
        
        <li class="menu-item menu-item-netty">
          <a href="/netty/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            netty
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.yifanguo.top/2019/01/07/SparkML源码分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yifan Guo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/timg.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yifan Guo Personal Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">SparkML源码分析</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-01-07T20:02:44+08:00">
                2019-01-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              

              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  24
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h1><span id="rdd-basic">RDD Basic</span></h1>
<h2><span id="transformation">Transformation</span></h2>
<p>rdd的操作分为transform和action</p>
<p>transform不会触发任务执行，action会</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>map</strong>(<em>func</em>)</td>
<td style="text-align:left">Return a new distributed dataset formed by passing each element of the source through a function <em>func</em>.</td>
</tr>
<tr>
<td><strong>filter</strong>(<em>func</em>)</td>
<td style="text-align:left">Return a new dataset formed by selecting those elements of the source on which <em>func</em>returns true.</td>
</tr>
<tr>
<td><strong>flatMap</strong>(<em>func</em>)</td>
<td style="text-align:left">Similar to map, but each input item can be mapped to 0 or more output items (so <em>func</em>should return a Seq rather than a single item).</td>
</tr>
<tr>
<td><strong>mapPartitions</strong>(<em>func</em>)</td>
<td style="text-align:left">Similar to map, but runs separately on each partition (block) of the RDD, so <em>func</em> must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</td>
</tr>
<tr>
<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>
<td style="text-align:left">Similar to mapPartitions, but also provides <em>func</em> with an integer value representing the index of the partition, so <em>func</em> must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.</td>
</tr>
<tr>
<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>
<td style="text-align:left">Sample a fraction <em>fraction</em> of the data, with or without replacement, using a given random number generator seed.</td>
</tr>
<tr>
<td><strong>union</strong>(<em>otherDataset</em>)</td>
<td style="text-align:left">Return a new dataset that contains the union of the elements in the source dataset and the argument.</td>
</tr>
<tr>
<td><strong>intersection</strong>(<em>otherDataset</em>)</td>
<td style="text-align:left">Return a new RDD that contains the intersection of elements in the source dataset and the argument.</td>
</tr>
<tr>
<td><strong>distinct</strong>([<em>numPartitions</em>]))</td>
<td style="text-align:left">Return a new dataset that contains the distinct elements of the source dataset.</td>
</tr>
<tr>
<td><strong>groupByKey</strong>([<em>numPartitions</em>])</td>
<td style="text-align:left">When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.  <strong>Note:</strong> If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better performance.  <strong>Note:</strong> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.</td>
</tr>
<tr>
<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td>
<td style="text-align:left">When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td>
</tr>
<tr>
<td><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</td>
<td style="text-align:left">When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument.</td>
</tr>
<tr>
<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td>
<td style="text-align:left">When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean <code>ascending</code> argument.</td>
</tr>
<tr>
<td><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td style="text-align:left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.</td>
</tr>
<tr>
<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td>
<td style="text-align:left">When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called <code>groupWith</code>.</td>
</tr>
<tr>
<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>
<td style="text-align:left">When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</td>
</tr>
<tr>
<td><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td>
<td style="text-align:left">Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.</td>
</tr>
<tr>
<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>
<td style="text-align:left">Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.</td>
</tr>
<tr>
<td><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td style="text-align:left">Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.</td>
</tr>
<tr>
<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>
<td style="text-align:left">Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within each partition because it can push the sorting down into the shuffle machinery.</td>
</tr>
</tbody>
</table>
<h2><span id="action">Action</span></h2>
<table>
<thead>
<tr>
<th>Action</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>reduce</strong>(<em>func</em>)</td>
<td>Aggregate the elements of the dataset using a function <em>func</em> (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.</td>
</tr>
<tr>
<td><strong>collect</strong>()</td>
<td>Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.</td>
</tr>
<tr>
<td><strong>count</strong>()</td>
<td>Return the number of elements in the dataset.</td>
</tr>
<tr>
<td><strong>first</strong>()</td>
<td>Return the first element of the dataset (similar to take(1)).</td>
</tr>
<tr>
<td><strong>take</strong>(<em>n</em>)</td>
<td>Return an array with the first <em>n</em> elements of the dataset.</td>
</tr>
<tr>
<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>
<td>Return an array with a random sample of <em>num</em> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td>
</tr>
<tr>
<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>
<td>Return the first <em>n</em> elements of the RDD using either their natural order or a custom comparator.</td>
</tr>
<tr>
<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>
<td>Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.</td>
</tr>
<tr>
<td><strong>saveAsSequenceFile</strong>(<em>path</em>)  (Java and Scala)</td>
<td>Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).</td>
</tr>
<tr>
<td><strong>saveAsObjectFile</strong>(<em>path</em>)  (Java and Scala)</td>
<td>Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using<code>SparkContext.objectFile()</code>.</td>
</tr>
<tr>
<td><strong>countByKey</strong>()</td>
<td>Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td>
</tr>
<tr>
<td><strong>foreach</strong>(<em>func</em>)</td>
<td>Run a function <em>func</em> on each element of the dataset. This is usually done for side effects such as updating an <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators">Accumulator</a> or interacting with external storage systems.  <strong>Note</strong>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-a-nameclosureslinka">Understanding closures </a>for more details.</td>
</tr>
</tbody>
</table>
<h2><span id="shuffle">shuffle</span></h2>
<p>shuffle就是洗牌的意思,意味着重新分配数据，我们看官方的解释：</p>
<p>Certain operations within Spark trigger an event known as the shuffle. The shuffle is Spark’s mechanism for re-distributing data so that it’s grouped differently across partitions. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation.</p>
<p>从reduceByKey来理解shuffle</p>
<p>thus, to organize all the data for a single <code>reduceByKey</code> reduce task to execute, Spark needs to perform an all-to-all operation. It must read from all partitions to find all the values for all keys, and then bring together values across partitions to compute the final result for each key - this is called the <strong>shuffle</strong>.</p>
<p>什么操作会触发shuffle</p>
<p>Operations which can cause a shuffle include <strong>repartition</strong> operations like <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#RepartitionLink"><code>repartition</code></a> and <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#CoalesceLink"><code>coalesce</code></a>, <strong>‘ByKey</strong> operations (except for counting) like <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#GroupByLink"><code>groupByKey</code></a> and <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#ReduceByLink"><code>reduceByKey</code></a>, and <strong>join</strong> operations like <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#CogroupLink"><code>cogroup</code></a> and <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#JoinLink"><code>join</code></a>.</p>
<p>shuffle操作非常昂贵：</p>
<p>The <strong>Shuffle</strong> is an expensive operation since it involves disk I/O, data serialization, and network I/O.</p>
<p>Internally, results from individual map tasks are kept in memory until they can’t fit. Then, these are sorted based on the target partition and written to a single file. On the reduce side, tasks read the relevant sorted blocks.</p>
<p>When data does not fit in memory Spark will spill these tables to disk, incurring the additional overhead of disk I/O and increased garbage collection.</p>
<p>shuffle当内存不够时会使用磁盘</p>
<p>Shuffle also generates a large number of intermediate files on disk. As of Spark 1.3, these files are preserved until the corresponding RDDs are no longer used and are garbage collected. This is done so the shuffle files don’t need to be re-created if the lineage is re-computed. Garbage collection may happen only after a long period of time, if the application retains references to these RDDs or if GC does not kick in frequently. This means that long-running Spark jobs may consume a large amount of disk space. The temporary storage directory is specified by the<code>spark.local.dir</code> configuration parameter when configuring the Spark context.</p>
<h2><span id="broadcast">broadcast</span></h2>
<p>Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.</p>
<h2><span id="accumulators">Accumulators</span></h2>
<p>Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel.</p>
<h1><span id="spark-streaming">Spark Streaming</span></h1>
<p>先看官方简介</p>
<p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</p>
<p>Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like <code>map</code>, <code>reduce</code>, <code>join</code> and <code>window</code>. Finally, processed data can be pushed out to filesystems, databases, and live dashboards.</p>
<p>其实spark streaming可以看成是间隔很小的批处理</p>
<p>虽然spark streaming是近似实时，但是throughput更大</p>
<h1><span id="latex">Latex</span></h1>
<p>$$  Ent(D) = - \sum_{i=1}^{|Y|} p_{k}\log_{2}p_{k}$$</p>
<h1><span id="决策树">决策树</span></h1>
<h2><span id="如何划分属性">如何划分属性</span></h2>
<p>在决策树算法中，如何选择最优划分属性是最关键的一步。一般而言，随着划分过程的不断进行，我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即节点的“纯度(purity)”越来越高。 有几种度量样本集合纯度的指标。在<code>MLlib</code>中，信息熵和基尼指数用于决策树分类，方差用于决策树回归。</p>
<h2><span id="信息熵">信息熵</span></h2>
<p>信息熵是度量样本集合纯度最常用的一种指标，假设当前样本集合<code>D</code>中第<code>k</code>类样本所占的比例为<code>p_k</code>，则<code>D</code>的信息熵定义为：</p>
<p>$$  Ent(D) = - \sum_{i=1}^{|Y|} p_{k}\log_{2}p_{k}$$</p>
<p><code>Ent(D)</code>的值越小，则<code>D</code>的纯度越高</p>
<h2><span id="gini基尼系数">Gini基尼系数</span></h2>
<p>$$  Gini(D)  = 1- \sum_{i=1}^{|Y|} p_{k}^2$$</p>
<p>直观来说，<code>Gini(D)</code>反映了从数据集<code>D</code>中随机取样两个样本，其类别标记不一致的概率。因此，<code>Gini(D)</code>越小，则数据集<code>D</code>的纯度越高。</p>
<h2><span id="方差">方差</span></h2>
<p>$$  Var(D)  = 1/N \sum_{i=1}^{N} (y_{i} - 1/N\sum_{i=1}^{N}y_{i})$$</p>
<p>这个也是sparkMLlib使用的方式</p>
<p>##信息增益</p>
<p>假设切分大小为<code>N</code>的数据集<code>D</code>为两个数据集<code>D_left</code>和<code>D_right</code>，那么信息增益可以表示为如下的形式</p>
<p>$$IG(D,s) = impurity(D) - \frac{N_{left}}{N} impurity(D)  -\frac{N_{right}}{N} impurity(D) $$</p>
<p>imformation gain</p>
<p>一般情况下，信息增益越大，则意味着使用属性<code>a</code>来进行划分所获得的纯度提升越大。因此我们可以用信息增益来进行决策树的划分属性选择。</p>
<h2><span id="决策树的缺点"><strong>决策树的缺点：</strong></span></h2>
<p>1.对那些各类别数据量不一致的数据，在决策树种，</p>
<p>信息增益的结果偏向那些具有更多数值的特征</p>
<p>2.容易过拟合</p>
<p>3.忽略了数据集中属性之间的相关性</p>
<h1><span id="随机森林">随机森林</span></h1>
<h2><span id="1-bagging">1 bagging</span></h2>
<p><code>Bagging</code>采用自助采样法(<code>bootstrap sampling</code>)采样数据。给定包含<code>m</code>个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时，样本仍可能被选中， 这样，经过<code>m</code>次随机采样操作，我们得到包含<code>m</code>个样本的采样集。</p>
<p>按照此方式，我们可以采样出<code>T</code>个含<code>m</code>个训练样本的采样集，然后基于每个采样集训练出一个基本学习器，再将这些基本学习器进行结合。这就是<code>Bagging</code>的一般流程。在对预测输出进行结合时，<code>Bagging</code>通常使用简单投票法， 对回归问题使用简单平均法。若分类预测时，出现两个类收到同样票数的情形，则最简单的做法是随机选择一个，也可以进一步考察学习器投票的置信度来确定最终胜者。</p>
<p><code>Bagging</code>的算法描述如下图所示。</p>
<p>输入： 训练集 D={(x1,y1),...}
基础学习算法 $$ B$$
训练次数T
过程：</p>
<ol>
<li>
<p>for t=1,2,...., T do</p>
</li>
<li>
<p>$$h_{t} =  B(D, D_{bs})$$</p>
</li>
<li>
<p>End for</p>
<p>输出：$$H_{x} = argmax {y}\in{Y} \sum_{t=1}^{T}I(h_{t}(x) = y)$$</p>
</li>
</ol>
<h2><span id="2-随机森林">2 随机森林</span></h2>
<p>随机森林是<code>Bagging</code>的一个扩展变体。随机森林在以决策树为基学习器构建<code>Bagging</code>集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来讲，传统决策树在选择划分属性时， 在当前节点的属性集合（假设有<code>d</code>个属性）中选择一个最优属性；而在随机森林中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含<code>k</code>个属性的子集，然后再从这个子集中选择一个最优属性用于划分。 这里的参数<code>k</code>控制了随机性的引入程度。若令<code>k=d</code>，则基决策树的构建与传统决策树相同；若令<code>k=1</code>，则是随机选择一个属性用于划分。在<code>MLlib</code>中，有两种选择用于分类，即<code>k=log2(d)</code>、<code>k=sqrt(d)</code>； 一种选择用于回归，即<code>k=1/3d</code>。</p>
<p>可以看出，随机森林对<code>Bagging</code>只做了小改动，但是与<code>Bagging</code>中基学习器的“多样性”仅仅通过样本扰动（通过对初始训练集采样）而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动。 这使得最终集成的泛化性能可通过个体学习器之间差异度的增加而进一步提升。</p>
<h2><span id="3-随机森林在分布式环境下的优化策略">3 随机森林在分布式环境下的优化策略</span></h2>
<ul>
<li>切分点抽样统计</li>
<li>特征装箱（<code>Binning</code>）</li>
<li>逐层训练（<code>level-wise training</code>）</li>
</ul>
<h1><span id="梯度提升树">梯度提升树</span></h1>
<h2><span id="1-boosting">1 boosting</span></h2>
<p><code>Boosting</code>是一类将弱学习器提升为强学习器的算法。这类算法的工作机制类似：先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注。 然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器的数目达到事先指定的值<code>T</code>，最终将这<code>T</code>个基学习器进行加权结合。</p>
<p><code>Boost</code>算法是在算法开始时，为每一个样本赋上一个相等的权重值，也就是说，最开始的时候，大家都是一样重要的。 在每一次训练中得到的模型，会使得数据点的估计有所差异，所以在每一步结束后，我们需要对权重值进行处理，而处理的方式就是通过<strong>增加错分点的权重</strong>，这样使得某些点如果老是被分错，那么就会被“严重关注”，也就被赋上一个很高的权重。 然后等进行了<code>N</code>次迭代，将会得到<code>N</code>个简单的基分类器（<code>basic learner</code>），最后将它们组合起来，可以对它们进行加权（错误率越大的基分类器权重值越小，错误率越小的基分类器权重值越大）、或者让它们进行投票等得到一个最终的模型。</p>
<p>梯度提升（<code>gradient boosting</code>）属于<code>Boost</code>算法的一种，也可以说是<code>Boost</code>算法的一种改进，它与传统的<code>Boost</code>有着很大的区别，它的每一次计算都是为了减少上一次的残差(<code>residual</code>)，而为了减少这些残差，可以在残差减少的梯度(<code>Gradient</code>)方向上建立一个新模型。所以说，在<code>Gradient Boost</code>中，每个新模型的建立是为了使得先前模型残差往梯度方向减少， 与传统的<code>Boost</code>算法对正确、错误的样本进行加权有着极大的区别。</p>
<p>梯度提升算法的核心在于，每棵树是从先前所有树的残差中来学习。<strong>利用的是当前模型中损失函数的负梯度值作为提升树算法中的残差的近似值</strong>，进而拟合一棵回归（分类）树。</p>
<p>##2 梯度提升</p>
<p><img src="/Users/yifanguo/Desktop/blog/source/_posts/SparkML%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/gbdt2.png" alt="gbdt2"></p>
<h2><span id="3-随机梯度提升">3 随机梯度提升</span></h2>
<p>有文献证明，注入随机性到上述的过程中可以提高函数估计的性能。受到<code>Breiman</code>的影响，将随机性作为一个考虑的因素。在每次迭代中，随机的在训练集中抽取一个子样本集，然后在后续的操作中用这个子样本集代替全体样本。</p>
<h1><span id="线性模型">线性模型</span></h1>
<h2><span id="1-数学描述">1 数学描述</span></h2>
<p>许多标准的机器学习算法可以归结为凸优化问题。例如，找到凸函数<code>f</code>的一个极小值的任务，这个凸函数依赖于可变向量<code>w</code>（在<code>spark</code>源码中，一般表示为<code>weights</code>）。 形式上，我们可以将其当作一个凸优化问题${min}_{w}f(w)$。它的目标函数可以表示为如下公式 <strong>(1)</strong>：</p>
<p>​	$$f(W) = \lambda R(W) + \frac{1}{n} \sum_{i=1}^{n}L(w; x_{i}, y{i})$$</p>
<p>在上式中，向量<code>x</code>表示训练数据集，<code>y</code>表示它相应的标签，也是我们想预测的值。如果<code>L(w;x,y)</code>可以表示为${w}^{T}x​$和<code>y</code>的函数， 我们称这个方法为线性的。<code>spark.mllib</code>中的几种分类算法和回归算法可以归为这一类。</p>
<p>目标函数<code>f</code>包含两部分：正则化(<code>regularizer</code>)，用于控制模型的复杂度；损失函数，用于度量模型的误差。损失函数<code>L(w;.)</code>是一个典型的基于<code>w</code>的凸函数。固定的正则化参数<code>gamma</code>定义了两种目标的权衡（<code>trade-off</code>）, 这两个目标分别是最小化损失(训练误差)以及最小化模型复杂度(为了避免过拟合)。</p>
<h1><span id="spark-ml整体架构">Spark ML整体架构</span></h1>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MLlib standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. This section covers the key concepts introduced by the Pipelines API, where the pipeline concept is mostly inspired by the scikit-learn project.</span><br><span class="line"></span><br><span class="line">DataFrame: This ML API uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types. E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.</span><br><span class="line"></span><br><span class="line">Transformer: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.</span><br><span class="line"></span><br><span class="line">Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.</span><br><span class="line"></span><br><span class="line">Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.</span><br><span class="line"></span><br><span class="line">Parameter: All Transformers and Estimators now share a common API for specifying parameters.</span><br></pre></td></tr></table></figure></p>
<p>sparkML基本上是按照上述介绍搭起来的，最基本的是PipelineStage 是Pipeline中的一步。</p>
<p>主要是estimator 和 transformer.</p>
<p>数据类型是dataFrame</p>
<h2><span id="estimator">Estimator</span></h2>
<p>estimator就一个method 就是fit</p>
<p>&lt;pre class=&quot;mermaid&quot;&gt;graph LR;
ProbabilisticClassifier --&gt; Classifier;
Classifier --&gt; Predictor;
Predictor --&gt; Estimator;&lt;/pre&gt;</p>
<p>ProbabilisticClassifier</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">class DecisionTreeClassifier @Since(&quot;1.4.0&quot;) (</span><br><span class="line">    @Since(&quot;1.4.0&quot;) override val uid: String)</span><br><span class="line">  extends ProbabilisticClassifier[Vector, DecisionTreeClassifier, DecisionTreeClassificationModel]</span><br></pre></td></tr></table></figure></p>
<h2><span id="transformer">Transformer</span></h2>
<p>transformer就是把一个df转成另一个df</p>
<h1><span id="logistic-regression">Logistic Regression</span></h1>
<p>spark的LR支持二分类和多分类，默认是二分类</p>
<p>看看LR的train都干了什么</p>
<p>首先LR支持样本有权重，只要df里有weight就行</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val instances: RDD[Instance] =</span><br><span class="line">      dataset.select(col($(labelCol)), w, col($(featuresCol))).rdd.map &#123;</span><br><span class="line">        case Row(label: Double, weight: Double, features: Vector) =&gt;</span><br><span class="line">          Instance(label, weight, features)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)</span><br></pre></td></tr></table></figure></p>
<p>这一步是把df里的label, weight, features都挑出来，如果没有weight 就默认是1</p>
<p>instances.persist 是确定数据的储存级别，默认就是MEMORY_AND_DISK</p>
<p>我们常用的rdd.cache() 本质上是 persist(StorageLevel.MEMORY_ONLY) 即把rdd的数据加载到内存</p>
<p>关于spark对内存的管理，可以看</p>
<p>https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html</p>
<p>https://www.jianshu.com/u/5b15278387a0</p>
<p>中间用的是Breeze的LBFGS优化器</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))</span><br></pre></td></tr></table></figure></p>
<p>最后组装model</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val model = copyValues(new LogisticRegressionModel(uid, coefficientMatrix, interceptVector,</span><br><span class="line">  numClasses, isMultinomial))</span><br></pre></td></tr></table></figure></p>
<h2><span id="treeaggregate">TreeAggregate</span></h2>
<p>treeAggregate也是spark rdd一个非常重要的操作，是数据聚合操作</p>
<p>可以看这篇</p>
<p>https://blog.csdn.net/u011724402/article/details/79057450</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与aggregate不同的是treeAggregate多了depth的参数，其他参数含义相同。aggregate在执行完SeqOp后会将计算结果拿到driver端使用CombOp遍历一次SeqOp计算的结果，最终得到聚合结果。而treeAggregate不会一次就Comb得到最终结果，SeqOp得到的结果也许很大，直接拉到driver可能会OutOfMemory，因此它会先把分区的结果做局部聚合(reduceByKey)，如果分区数过多时会做分区合并，之后再把结果拿到driver端做reduce</span><br></pre></td></tr></table></figure></p>
<h2><span id="gradientdescent">GradientDescent</span></h2>
<p>所有在spark上实现并行的ML的算法都可以参考梯度下降的做法</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">while (!converged &amp;&amp; i &lt;= numIterations) &#123;</span><br><span class="line">	// 把权重分发给executor, 每轮迭代都要发一次！？这就是spark的瓶颈</span><br><span class="line">  val bcWeights = data.context.broadcast(weights)</span><br><span class="line">  // Sample a subset (fraction miniBatchFraction) of the total data</span><br><span class="line">  // compute and sum up the subgradients on this subset (this is one map-reduce)</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  val (gradientSum, lossSum, miniBatchSize) = data.sample(false, miniBatchFraction, 42 + i)</span><br><span class="line">    .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(</span><br><span class="line">      seqOp = (c, v) =&gt; &#123;</span><br><span class="line">        // c: (grad, loss, count), v: (label, features)</span><br><span class="line">        val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))</span><br><span class="line">        (c._1, c._2 + l, c._3 + 1)</span><br><span class="line">      &#125;,</span><br><span class="line">      combOp = (c1, c2) =&gt; &#123;</span><br><span class="line">        // c: (grad, loss, count)</span><br><span class="line">        (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)</span><br><span class="line">      &#125;)</span><br><span class="line">  bcWeights.destroy(blocking = false)</span><br><span class="line"></span><br><span class="line">  if (miniBatchSize &gt; 0) &#123;</span><br><span class="line">    /**</span><br><span class="line">     * lossSum is computed using the weights from the previous iteration</span><br><span class="line">     * and regVal is the regularization value computed in the previous iteration as well.</span><br><span class="line">     */</span><br><span class="line">    stochasticLossHistory += lossSum / miniBatchSize + regVal</span><br><span class="line">    val update = updater.compute(</span><br><span class="line">      weights, Vectors.fromBreeze(gradientSum / miniBatchSize.toDouble),</span><br><span class="line">      stepSize, i, regParam)</span><br><span class="line">    weights = update._1</span><br><span class="line">    regVal = update._2</span><br><span class="line"></span><br><span class="line">    previousWeights = currentWeights</span><br><span class="line">    currentWeights = Some(weights)</span><br><span class="line">    if (previousWeights != None &amp;&amp; currentWeights != None) &#123;</span><br><span class="line">      converged = isConverged(previousWeights.get,</span><br><span class="line">        currentWeights.get, convergenceTol)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    logWarning(s&quot;Iteration ($i/$numIterations). The size of sampled batch is zero&quot;)</span><br><span class="line">  &#125;</span><br><span class="line">  i += 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2><span id="rdd-sampling">RDD sampling</span></h2>
<p>RDD提供了sampling的接口</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Return a sampled subset of this RDD.</span><br><span class="line">  *</span><br><span class="line">  * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span><br><span class="line">  * @param fraction expected size of the sample as a fraction of this RDD&apos;s size</span><br><span class="line">  *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span><br><span class="line">  *  with replacement: expected number of times each element is chosen; fraction must be greater</span><br><span class="line">  *  than or equal to 0</span><br><span class="line">  * @param seed seed for the random number generator</span><br><span class="line">  *</span><br><span class="line">  * @note This is NOT guaranteed to provide exactly the fraction of the count</span><br><span class="line">  * of the given [[RDD]].</span><br><span class="line">  */</span><br><span class="line"> def sample(</span><br><span class="line">     withReplacement: Boolean,</span><br><span class="line">     fraction: Double,</span><br><span class="line">     seed: Long = Utils.random.nextLong): RDD[T] = &#123;</span><br><span class="line">   require(fraction &gt;= 0,</span><br><span class="line">     s&quot;Fraction must be nonnegative, but got $&#123;fraction&#125;&quot;)</span><br><span class="line"></span><br><span class="line">   withScope &#123;</span><br><span class="line">     require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction)</span><br><span class="line">     if (withReplacement) &#123;</span><br><span class="line">       new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">       new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark/" rel="tag"># spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/01/02/LightGBM模型解读/" rel="next" title="LightGBM模型解读">
                <i class="fa fa-chevron-left"></i> LightGBM模型解读
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/01/16/YarnIsSimple/" rel="prev" title="YarnIsSimple">
                YarnIsSimple <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/timg.jpeg"
                alt="Yifan Guo" />
            
              <p class="site-author-name" itemprop="name">Yifan Guo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">64</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">36</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">1.</span> <span class="nav-text">RDD Basic</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.1.</span> <span class="nav-text">Transformation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.2.</span> <span class="nav-text">Action</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.3.</span> <span class="nav-text">shuffle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.4.</span> <span class="nav-text">broadcast</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">1.5.</span> <span class="nav-text">Accumulators</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">2.</span> <span class="nav-text">Spark Streaming</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">3.</span> <span class="nav-text">Latex</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">4.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.1.</span> <span class="nav-text">如何划分属性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.2.</span> <span class="nav-text">信息熵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.3.</span> <span class="nav-text">Gini基尼系数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.4.</span> <span class="nav-text">方差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">4.5.</span> <span class="nav-text">决策树的缺点：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">5.</span> <span class="nav-text">随机森林</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">5.1.</span> <span class="nav-text">1 bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">5.2.</span> <span class="nav-text">2 随机森林</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">5.3.</span> <span class="nav-text">3 随机森林在分布式环境下的优化策略</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">6.</span> <span class="nav-text">梯度提升树</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">6.1.</span> <span class="nav-text">1 boosting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">6.2.</span> <span class="nav-text">3 随机梯度提升</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">7.</span> <span class="nav-text">线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">7.1.</span> <span class="nav-text">1 数学描述</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">8.</span> <span class="nav-text">Spark ML整体架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">8.1.</span> <span class="nav-text">Estimator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">8.2.</span> <span class="nav-text">Transformer</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#undefined"><span class="nav-number">9.</span> <span class="nav-text">Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">9.1.</span> <span class="nav-text">TreeAggregate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">9.2.</span> <span class="nav-text">GradientDescent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#undefined"><span class="nav-number">9.3.</span> <span class="nav-text">RDD sampling</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yifan Guo</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">72.5k</span>
  

  
  <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js'></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize("");
    }
  </script>
  

</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://www.yifanguo.top/2019/01/07/SparkML源码分析/';
          this.page.identifier = '2019/01/07/SparkML源码分析/';
          this.page.title = 'SparkML源码分析';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://www-yifanguo-top.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  





  

  

  

  
  

  

  

  

  
</body>
</html>
