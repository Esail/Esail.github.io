<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yifan Guo Personal Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.yifanguo.top/"/>
  <updated>2018-12-19T06:12:34.000Z</updated>
  <id>http://www.yifanguo.top/</id>
  
  <author>
    <name>Yifan Guo</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>test</title>
    <link href="http://www.yifanguo.top/2018/12/19/test/"/>
    <id>http://www.yifanguo.top/2018/12/19/test/</id>
    <published>2018-12-19T06:00:15.000Z</published>
    <updated>2018-12-19T06:12:34.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="test">test</span></h1><p><img src="./test/pslite.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;test&quot;&gt;test&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;./test/pslite.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>pslite</title>
    <link href="http://www.yifanguo.top/2018/12/19/pslite/"/>
    <id>http://www.yifanguo.top/2018/12/19/pslite/</id>
    <published>2018-12-19T01:58:03.000Z</published>
    <updated>2018-12-19T06:01:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><p>https://zhuanlan.zhihu.com/p/48794558http://www.cnblogs.com/heguanyou/p/7868596.htmlhttps://blog.csdn.net/KangRoger/article/details/73307685</p><p>https://blog.csdn.net/xln0130/article/details/5696001https://www.jianshu.com/p/d3e503ddd68ehttps://www.zybuluo.com/Dounm/note/517675</p><h1><span id="rpc-vs-socket">RPC vs socket</span></h1><p>两个老板手下各有一个负责接通MSN的秘书.这两个秘书就是基于RPC协议建立的会话层通信.老本不需要知道怎么使用MSN,只要告诉秘书,秘书就会通过MSN与对方建立会话请求和响应.而基于Socket的通信,老板需要会使用MSN,这样尽管老板需要事先培训一下MSN的简单使用常识,但若与对方通信时,无需经过秘书,效率更高.</p><h1><span id="pslite全解析">Pslite全解析</span></h1><p><img src="./pslite/pslite.jpg" alt=""></p><ul><li>虚线实心箭头：依赖关系</li><li>实心菱形： 合成关系，simpleApp中有一个customer成员</li><li>空心三角+实线： 继承关系</li></ul><p><img src="./pslite/class.png" alt=""></p><h2><span id="postoffice">PostOffice</span></h2><p>全局管理类,每个node只有一个该类的对象</p><p>主要用来配置当前node的一些信息，例如当前node是哪种类型(server,worker,scheduler)，nodeid是啥，以及worker/server 的rank 到 node id的转换。</p><p>Postoffice类利用Singleton模式来保证只有一个对象。</p><h2><span id="van">Van</span></h2><p>核心通信类，每个节点只有一个该对象，是Postoffice对象的成员。</p><p>Van类负责建立起节点之间的互相连接（例如Worker与Scheduler之间的连接），并且开启本地的receiving thread用来监听收到的message。</p><p>Van的子类为ZMQVan，即为用zmq库实现了连接的底层细节（zmq库是一个开源库，对socket进行了优良的封装</p><h2><span id="customer">Customer</span></h2><p>用于通信的对象。</p><p>每个Customer都与某个node id相绑定，代表当前节点发送到对应node id节点。</p><p>Customer对象维护request和response的状态，其中tracker_成员记录每个请求可能发送给了多少节点以及从多少个节点返回。tracker_ 下标为每个req标识的timestamp。</p><p>Customer也会启动一个receiving thread，而它接受到的消息来自于Van的receiving thread，即每个节点的Van对象收到message后，根据message的不同，推送到不同的customer对象中。</p><h2><span id="simpleapp">SimpleApp</span></h2><p>简单的通信类。每次通信发送int型的head和string型的body。</p><p>SimpleApp对象中包括一个Customer对象用来控制请求连接。</p><h2><span id="kvworker">KVWorker</span></h2><p>继承自SimpleApp，包括如下方法： Push(),Pull(),Wait()。</p><p>Push()和Pull()最后都会调用Send()函数，Send()对KVPairs进行切分，因为每个Server只保留一部分参数，因此切分后的SlicedKVpairs就会被发送给不同的Server。</p><p>切分函数可以由用户自行重写，默认为DefaultSlicer，每个SlicedKVPairs被包装成Message对象，然后用van::send()发送。</p><h2><span id="kvserver">KVServer</span></h2><p>继承自SimpleApp，包含如下方法：Process()和Response()。</p><p>Process()被注册到Customer对象中，当Customer对象的receiving thread接受到消息时，就调用Process()对数据进行处理。Process()内部的逻辑是调用 用户自行实现的一个std::function函数对象 对数据进行处理。</p><p>Response()就仅仅是向调用的worker发送信息</p><h2><span id="kvpairs">KVPairs</span></h2><p>拥有keys, values, lens等3个数组。lens和keys大小相等，表示每个key对应的value的个数。lens可为空，此时values被平分。</p><p>举例而言，若keys=[1,5]，lens=[2,3]，那么values[0],values[1]就对应的是keys[0]，而values[2],values[3],values[5]对应的就是keys[1]。而如果len为空，则values.size()必须是keys.size()（此处为2）的倍数，key[0]和key[1]各对应一半的values。</p><h2><span id="sarray">SArray</span></h2><p>shared array，用std::shared_ptr实现的数组，用于替代std::vector，避免数组的深拷贝。</p><p>#消息处理流程</p><p>无论是worker节点还是server节点，在程序的最开始都会执行Postoffice::start()。Postoffice::start()会初始化节点信息，并且调用Van::start()。而Van::start()则会让当前节点与Scheduler节点相连，并且启动一个本地线程recv thread来持续监听收到的message。</p><p>worker和server都继承自SimpleApp类，所以都有一个customer对象。customer对象本身也会启动一个recv thread，其中调用注册的recv_handle_函数对消息进行处理。</p><p>对于worker来说，其注册的recv_handle_是KVWorker::Process()函数。因为worker的recv thread接受到的消息主要是从server处pull下来的KV对，因此该Process()主要是接收message中的KV对；</p><p>而对于Server来说，其注册的recv_handle_是KVServer::Process()函数。因此server接受的是worker们push上来的KV对，需要对其进行处理，因此该Process()函数中调用的用户通过KVServer::set_request_handle()传入的函数对象。</p><p>每个customer对象都拥有一个tracker_(std::vector&lt;std::pair&lt;int, int&gt;&gt;类型)用来记录每个请求发送和返回的数量。tracker_的下标即为请求的timestamp，tracker_[t].first是该请求发送给了多少节点，tracker[t]<em>.second是该请求收到了多少节点的回复。customer::Wait()就是一直阻塞直到tracker</em>[t].first == tracker[t].second，用这个来控制同步异步依赖。</p><p>每当Van的recv thread收到一个message时，就会根据customer id的不同将message发给不同的customer的recv thread。同时该message对应的请求（设为req）则tracker_[req.timestamp].second++</p><h1><span id="node">Node</span></h1><p><img src="./pslite/node.png" alt=""></p><p>真的是非常巧妙的表示，学到了</p><h1><span id="消息封装">消息封装</span></h1><p><img src="./pslite/msg.png" alt=""></p><p>Node 存放ip port id等信息Control存放command类型 command=BARRIERMeta timeStamp, 发送者id,接受者id, 控制信息controlMessage. 消息头Meta, 消息体data</p><h1><span id="node之间协同工作">Node之间协同工作</span></h1><p><img src="./pslite/node_work.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;p&gt;https://zhuanlan.zhihu.com/p/48794558
http://www.cnblogs.com/heguanyou/p/7868596.html
https://blog.csdn.net/KangRoger/articl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>parameter_server</title>
    <link href="http://www.yifanguo.top/2018/10/29/parameter-server/"/>
    <id>http://www.yifanguo.top/2018/10/29/parameter-server/</id>
    <published>2018-10-28T19:53:51.000Z</published>
    <updated>2018-10-30T02:32:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="parameter-server技术内幕">parameter server技术内幕</span></h1><p>深入解析parameter server架构设计与实现原理</p><h1><span id="第一章-回顾分布式机器学习的历史进程">第一章 回顾分布式机器学习的历史进程</span></h1><h2><span id="1背景">1.背景</span></h2><p>自从 google 发表著名的 GFS、MapReduce、BigTable 三篇 paper 以后，互联网正式迎来了大数据时代。大数据的显著特点是大，哪里都大的大。本篇主要针对 volume 大的数据时，使用机器学习来进行数据处理过程中遇到的架构方面的问题做一个系统的梳理。</p><p>有了 GFS 我们有能力积累海量的数据样本，比如在线广告的曝光和点击数据，天然具有正负样本的特性，累积一两个月往往就能轻松获得百亿、千亿级的训练样本。这样海量的样本如何存储？用什么样的模型可以学习海量样本中有用的 pattern？这些问题不止是工程问题，也值得每个做算法的同学去深入思考。</p><h3><span id="11-简单模型-or-复杂模型">1.1 简单模型 or 复杂模型</span></h3><p>在深度学习概念提出之前，算法工程师手头能用的工具其实并不多，就 LR、SVM、感知机等寥寥可数、相对固定的若干个模型和算法；那时候要解决一个实际的问题，算法工程师更多的工作主要是在特征工程方面。而特征工程本身并没有很系统化的指导理论（至少目前没有看到系统介绍特征工程的书籍），所以很多时候特征的构造技法显得光怪陆离，是否有用也取决于问题本身、数据样本、模型以及运气。</p><p>在特征工程作为算法工程师主要工作内容的时候，构造新特征的尝试往往很大部分都不能在实际工作中发挥作用。据我了解，国内几家大公司在特征构造方面的成功率在后期一般不会超过 20%。也就是 80% 的新构造特征往往并没什么正向提升效果。如果给这种方式起一个名字的话，大概是简单模型 + 复杂特征。简单模型说的是算法比如 LR、SVM 本身并不复杂，参数和表达能力基本呈现一种线性关系，易于理解。复杂特征则是指特征工程方面不断尝试使用各种奇技淫巧构造的可能有用、可能没用的特征，这部分特征的构造方式可能会有各种 trick，比如窗口滑动、离散化、归一化、开方、平方、笛卡尔积、多重笛卡尔积等等；顺便提一句，因为特征工程本身并没有特别系统的理论和总结，所以初入行的同学想要构造特征就需要多读 paper，特别是和自己业务场景一样或类似的场景的 paper，从里面学习作者分析、理解数据的方法以及对应的构造特征的技法；久而久之，有望形成自己的知识体系。</p><p>深度学习概念提出以后，人们发现通过深度神经网络可以进行一定程度的表示学习（representation learning）。例如在图像领域，通过 CNN 提取图像 feature 并在此基础上进行分类的方法，一举打破了之前算法的天花板，而且是以极大的差距打破。这给所有算法工程师带来了新的思路，既然深度学习本身有提取特征的能力，干嘛还要苦哈哈的自己去做人工特征设计呢？</p><p>深度学习虽然一定程度上缓解了特征工程的压力，但这里要强调两点：</p><ol><li><p>缓解并不等于彻底解决，除了图像这种特定领域，在个性化推荐等领域，深度学习目前还没有完全取得绝对的优势。究其原因，可能还是数据自身内在结构的问题，使得在其他领域目前还没有发现类似图像+CNN 这样的完美 comple。</p></li><li><p>深度学习在缓解特征工程的同时，也带来了模型复杂、不可解释的问题。算法工程师在网络结构设计方面一样要花很多心思来提升效果。概括起来，深度学习代表的简单特征 + 复杂模型是解决实际问题的另一种方式。</p></li></ol><p>两种模式孰优孰劣还难有定论，以点击率预测为例，在计算广告领域往往以海量特征 +LR 为主流，根据 VC 维理论，LR 的表达能力和特征个数成正比，因此海量的 feature 也完全可以使 LR 拥有足够的描述能力。而在个性化推荐领域，深度学习刚刚萌芽，目前 google play 采用了 WDL 的结构 [1]，youtube 采用了双重 DNN 的结构 [2]。</p><p>不管是那种模式，当模型足够庞大的时候，都会出现模型参数一台机器无法存放的情况。比如百亿级 feature 的 LR 对应的权重 w 有好几十个 G，这在很多单机上存储都是困难的，大规模神经网络则更复杂，不仅难以单机存储，而且参数和参数之间还有逻辑上的强依赖；要对超大规模的模型进行训练势必要借用分布式系统的技法，本文主要是系统总结这方面的一些思路。</p><h3><span id="12-数据并行-vs-模型并行">1.2 数据并行 vs 模型并行</span></h3><p>数据并行和模型并行是理解大规模机器学习框架的基础概念，其缘起未深究，第一次看到是在姐夫（Jeff Dean）的 blog 里，当时匆匆一瞥，以为自己懂了。多年以后，再次开始调研这个问题的时候才想起长者的教训，年轻人啊，还是图样，图森破。如果你和我一样曾经忽略过这个概念，今天不妨复习一下。</p><p>这两个概念在 [3] 中沐帅曾经给出了一个非常直观而经典的解释，可惜不知道什么原因，当我想引用时却发现已经被删除了。我在这里简单介绍下这个比喻：如果要修两栋楼，有一个工程队，怎么操作？第一个方案是将人分成两组，分别盖楼，盖好了就装修；第二种做法是一组人盖楼，等第一栋楼盖好，另一组装修第一栋，然后第一组继续盖第二栋楼，改完以后等装修队装修第二栋楼。乍一看，第二种方法似乎并行度并不高，但第一种方案需要每个工程人员都拥有“盖楼”和“装修”两种能力，而第二个方案只需要每个人拥有其中一种能力即可。第一个方案和数据并行类似，第二个方案则道出了模型并行的精髓。</p><p>数据并行理解起来比较简单，当样本比较多的时候，为了使用所有样本来训练模型，我们不妨把数据分布到不同的机器上，然后每台机器都来对模型参数进行迭代，如下图所示。</p><p><img src="sdp.png" alt=""></p><p>图片取材于 TensorFlow 的 paper[4]，图中 ABC 代表三台不同的机器，上面存储着不同的样本，模型 P 在各台机器上计算对应的增量，然后在参数存储的机器上进行汇总和更新，这就是数据并行。先忽略 synchronous，这是同步机制相关的概念，在第三节会有专门介绍。</p><p>数据并行概念简单，而且不依赖于具体的模型，因此数据并行机制可以作为框架的一种基础功能，对所有算法都生效。与之不同的是，模型并行因为参数间存在依赖关系（其实数据并行参数更新也可能会依赖所有的参数，但区别在于往往是依赖于上一个迭代的全量参数。而模型并行往往是同一个迭代内的参数之间有强依赖关系，比如 DNN 网络的不同层之间的参数依照 BP 算法形成的先后依赖），无法类比数据并行这样直接将模型参数分片而破坏其依赖关系，所以模型并行不仅要对模型分片，同时需要调度器来控制参数间的依赖关系。而每个模型的依赖关系往往并不同，所以模型并行的调度器因模型而异，较难做到完全通用。关于这个问题，CMU 的 Erix Xing 在 [5] 中有所介绍，感兴趣的可以参考。</p><p>模型并行的问题定义可以参考姐夫的 [6]，这篇 paper 也是 tensorflow 的前身相关的总结，其中如下图：</p><p><img src="tf_ps.png" alt=""></p><p>解释了模型并行的物理图景，当一个超大神经网络无法存储在一台机器上时，我们可以切割网络存到不同的机器上，但是为了保持不同参数分片之间的依赖，如图中粗黑线的部分，则需要在不同的机器之间进行 concurrent 控制；同一个机器内部的参数依赖，即图中细黑线部分在机器内即可完成控制。</p><p>黑线部分如何有效控制呢？如下图所示：</p><p><img src="tf_ps_2.png" alt=""></p><p>在将模型切分到不同机器以后，我们将参数和样本一起在不同机器间流转，图中 ABC 代表模型的不同部分的参数；假设 C 依赖 B，B 依赖 A，机器 1 上得到 A 的一个迭代后，将 A 和必要的样本信息一起传到机器 2，机器 2 根据 A 和样本对 P2 更新得到，以此类推；当机器 2 计算 B 的时候，机器 1 可以展开 A 的第二个迭代的计算。了解 CPU 流水线操作的同学一定感到熟悉，是的，模型并行是通过数据流水线来实现并行的。想想那个盖楼的第二种方案，就能理解模型并行的精髓了。</p><p><img src="tf_ps_3.png" alt=""></p><p>上图则是对控制模型参数依赖的调度器的一个示意图，实际框架中一般都会用 DAG（有向无环图）调度技术来实现类似功能，未深入研究，以后有机会再补充说明。</p><p>理解了数据并行和模型并行对后面参数服务器的理解至关重要，但现在让我先荡开一笔，简单介绍下并行计算框架的一些背景信息。</p><h2><span id="2并行算法演进">2.并行算法演进</span></h2><h3><span id="21-mapreduce-路线">2.1 MapReduce 路线</span></h3><p>从函数式编程中受到启发，Google 发布了 MapReduce[7] 的分布式计算方式；通过将任务切分成多个叠加的 Map+Reduce 任务，来完成复杂的计算任务，示意图如下：</p><p><img src="mr.png" alt=""></p><p>MapReduce 的主要问题有两个，一是原语的语义过于低级，直接使用其来写复杂算法，开发量比较大；另一个问题是依赖于磁盘进行数据传递，性能跟不上业务需求。</p><p>为了解决 MapReduce 的两个问题，Matei 在 [8] 中提出了一种新的数据结构 RDD，并构建了 Spark 框架。Spark 框架在 MR 语义之上封装了 DAG 调度器，极大降低了算法使用的门槛。较长时间内 Spark 几乎可以说是大规模机器学习的代表，直至后来沐帅的参数服务器进一步开拓了大规模机器学习的领域以后，Spark 才暴露出一点点不足。如下图：</p><p><img src="spark.png" alt=""></p><p>从图中可以看出，Spark 框架以 Driver 为核心，任务调度和参数汇总都在 Driver，而 Driver 是单机结构，所以 Spark 的瓶颈非常明显，就在 Driver 这里。当模型规模大到一台机器存不下的时候，Spark 就无法正常运行了。所以从今天的眼光来看，Spark 只能称为一个中等规模的机器学习框架。</p><p>MapReduce 不仅是一个框架，还是一种思想，Google 开创性的工作为我们找到了大数据分析的一个可行方向，时至今日，仍不过时。只是逐渐从业务层下沉到底层语义应该处于的框架下层。</p><h3><span id="22-mpi">2.2 MPI</span></h3><p>沐帅在 [9] 中对 MPI 的前景做了简要介绍；和 Spark 不同，MPI 是类似 socket 的一种系统通信 API，只是支持了消息广播等功能。因为对 MPI 研究不深入，这里简单介绍下优点和缺点吧。优点是系统级支持，性能杠杠的；缺点也比较多，一是和 MR 一样因为原语过于低级，用 MPI 写算法，往往代码量比较大；另一方面是基于 MPI 的集群，如果某个任务失败，往往需要重启整个集群，而 MPI 集群的任务成功率并不高。阿里在 [10] 中给出了下图：</p><p><img src="mpi.png" alt=""></p><p>从图中可以看出，MPI 作业失败的几率接近五成。MPI 也并不是完全没有可取之处，正如沐帅所说，在超算集群上还是有场景的。对于工业届依赖于云计算、依赖于 commodity 计算机来说，则显得性价比不够高。当然如果在参数服务器的框架下，对单组 worker 再使用 MPI 未尝不是个好的尝试，[10] 的鲲鹏系统正是这么设计的。</p><h2><span id="3-参数服务器演进">3. 参数服务器演进</span></h2><p>沐帅在 [12] 中将参数服务器的历史划分为三个阶段，第一代参数服务器萌芽于沐帅的导师 Smola 的 [11]，如下图所示：</p><p><img src="ps_h1.png" alt=""></p><p>这个工作中仅仅引入 memcached 来存放 key-value 数据，不同的处理进程并行对其进行处理。[13] 中也有类似的想法，第二代参数服务器叫 application-specific 参数服务器，主要针对特定应用而开发，其中最典型的代表应该是 TensorFlow 的前身 [6]。</p><p>第三代参数服务器，也即是通用参数服务器框架是由百度少帅李沐正式提出的，和前两代不同，第三代参数服务器从设计上就是作为一个通用大规模机器学习框架来定位的。要摆脱具体应用、算法的束缚，做一个通用的大规模机器学习框架，首先就要定义好框架的功能；而所谓框架，往往就是把大量重复的、琐碎的、做了一次就不想再来第二次的脏活、累活进行良好而优雅的封装，让使用框架的人可以只关注于自己的核心逻辑。第三代参数服务器要对那些功能进行封装呢？沐帅总结了这几点，我照搬如下：</p><p>1）高效的网络通信：因为不管是模型还是样本都十分巨大，因此对网络通信的高效支持以及高配的网络设备都是大规模机器学习系统不可缺少的；</p><p>2）灵活的一致性模型：不同的一致性模型其实是在模型收敛速度和集群计算量之间做 tradeoff；要理解这个概念需要对模型性能的评价做些分析，暂且留到下节再介绍。</p><p>3）弹性可扩展：显而易见</p><p>4）容灾容错：大规模集群协作进行计算任务的时候，出现 Straggler 或者机器故障是非常常见的事，因此系统设计本身就要考虑到应对；没有故障的时候，也可能因为对任务时效性要求的变化而随时更改集群的机器配置。这也需要框架能在不影响任务的情况下能做到机器的热插拔。</p><p>5）易用性：主要针对使用框架进行算法调优的工程师而言，显然，一个难用的框架是没有生命力的。</p><p>在正式介绍第三代参数服务器的主要技术之前，先从另一个角度来看下大规模机器学习框架的演进。</p><p><img src="ps_h2.png" alt=""></p><p>这张图可以看出，在参数服务器出来之前，人们已经做了多方面的并行尝试，不过往往只是针对某个特定算法或特定领域，比如 YahooLDA 是针对 LDA 算法的。当模型参数突破十亿以后，则可以看出参数服务器一统江湖，再无敌手。</p><p>首先我们看看第三代参数服务器的基本架构。</p><p><img src="ps_h3.png" alt=""></p><p>上图的 resource manager 可以先放一放，因为实际系统中这部分往往是复用现有的资源管理系统，比如 yarn 或者 mesos；底下的 training data 毋庸置疑的需要类似 GFS 的分布式文件系统的支持；剩下的部分就是参数服务器的核心组件了。</p><p>图中画了一个 server group 和三个 worker group；实际应用中往往也是类似，server group 用一个，而 worker group 按需配置；server manager 是 server group 中的管理节点，一般不会有什么逻辑，只有当有 server node 加入或退出的时候，为了维持一致性哈希而做一些调整。</p><p>Worker group 中的 task schedule 则是一个简单的任务协调器，一个具体任务运行的时候，task schedule 负责通知每个 worker 加载自己对应的数据，然后去 server node 上拉取一个要更新的参数分片，用本地数据样本计算参数分片对应的变化量，然后同步给 server node；server node 在收到本机负责的参数分片对应的所有 worker 的更新后，对参数分片做一次 update。</p><p><img src="ps_h4.png" alt=""></p><p>如图所示，不同的 worker 同时并行运算的时候，可能因为网络、机器配置等外界原因，导致不同的 worker 的进度是不一样的，如何控制 worker 的同步机制是一个比较重要的课题。详见下节分解。</p><h2><span id="32-同步协议">3.2 同步协议</span></h2><p>本节假设读者已经对随机梯度优化算法比较熟悉，如果不熟悉的同学请参考吴恩达经典课程机器学习中对 SGD 的介绍，或者我之前多次推荐过的书籍《最优化导论》。</p><p>我们先看一个单机算法的运行过程，假设一个模型的参数切分成三个分片 k1，k2，k3；比如你可以假设是一个逻辑回归算法的权重向量被分成三段。我们将训练样本集合也切分成三个分片 s1，s2，s3；在单机运行的情况下，我们假设运行的序列是（k1，s1）、（k2，s1）、（k3、s1）、（k1、s2）、（k2、s2）、（k3、s2）……看明白了吗？就是假设先用 s1 中的样本一次对参数分片 k1、k2、k3 进行训练，然后换 s2；这就是典型的单机运行的情况，而我们知道这样的运行序列最后算法会收敛。</p><p>现在我们开始并行化，假设 k1、k2、k3 分布在三个 server node 上，s1、s2、s3 分布在三个 worker 上，这时候如果我们还要保持之前的计算顺序，则会变成怎样？work1 计算的时候，work2 和 worker3 只能等待，同样 worker2 计算的时候，worker1 和 work3 都得等待，以此类推；可以看出这样的并行化并没有提升性能；但是也算简单解决了超大规模模型的存储问题。</p><p>为了解决性能的问题，业界开始探索这里的一致性模型，最先出来的版本是前面提到的 [11] 中的 ASP 模式，就是完全不顾 worker 之间的顺序，每个 worker 按照自己的节奏走，跑完一个迭代就 update，然后继续，这应该是大规模机器学习中的 freestyle 了，如图所示：</p><p><img src="asp.png" alt="">ASP 的优势是最大限度利用了集群的计算能力，所有的 worker 所在的机器都不用等待，但缺点也显而易见，除了少数几个模型，比如 LDA，ASP 协议可能导致模型无法收敛。也就是 SGD 彻底跑飞了，梯度不知道飞到哪里去了。</p><p>在 ASP 之后提出了另一种相对极端的同步协议 BSP，Spark 用的就是这种方式，如图所示：</p><p><img src="bsp.png" alt=""></p><p>每个 worker 都必须在同一个迭代运行，只有一个迭代任务所有的 worker 都完成了，才会进行一次 worker 和 server 之间的同步和分片更新。这个算法和严格一直的算法非常类似，区别仅仅在于单机版本的 batch size 在 BSP 的时候变成了有所有 worker 的单个 batch size 求和得到的总的 butch size 替换。毫无疑问，BSP 的模式和单机串行因为仅仅是 batch size 的区别，所以在模型收敛性上是完全一样的。同时，因为每个 worker 在一个周期内是可以并行计算的，所以有了一定的并行能力。</p><p>以此协议为基础的 Spark 在很长时间内成为机器学习领域实际的霸主，不是没有理由的。此种协议的缺陷之处在于，整个 worker group 的性能由其中最慢的 worker 决定；这个 worker 一般称为 straggler。读过 GFS 文章的同学应该都知道 straggler 的存在是非常普遍的现象。</p><p>能否将 ASP 和 BSP 做一下折中呢？答案当然是可以的，这就是目前我认为最好的同步协议 SSP；SSP 的思路其实很简单，既然 ASP 是允许不同 worker 之间的迭代次数间隔任意大，而 BSP 则只允许为 0，那我是否可以取一个常数 s？如图所示：</p><p>能否将 ASP 和 BSP 做一下折中呢？答案当然是可以的，这就是目前我认为最好的同步协议 SSP；SSP 的思路其实很简单，既然 ASP 是允许不同 worker 之间的迭代次数间隔任意大，而 BSP 则只允许为 0，那我是否可以取一个常数 s？如图所示：</p><p><img src="ssp.png" alt=""></p><p>不同的 worker 之间允许有迭代的间隔，但这个间隔数不允许超出一个指定的数值 s，图中 s=3.</p><p>SSP 协议的详细介绍参见 [14]，CMU 的大拿 Eric Xing 在其中详细介绍了 SSP 的定义，以及其收敛性的保证。理论推导证明常数 s 不等于无穷大的情况下，算法一定可以在若干次迭代以后进入收敛状态。其实在 Eric 提出理论证明之前，工业界已经这么尝试过了：）</p><p>顺便提一句，考察分布式算法的性能，一般会分为 statistical performance 和 hard performance 来看。前者指不同的同步协议导致算法收敛需要的迭代次数的多少，后者是单次迭代所对应的耗时。两者的关系和 precision\recall 关系类似，就不赘述了。有了 SSP，BSP 就可以通过指定 s=0 而得到。而 ASP 同样可以通过制定 s=∞来达到。</p><h2><span id="33-核心技术">3.3 核心技术</span></h2><p>除了参数服务器的架构、同步协议之外，本节再对其他技术做一个简要的介绍，详细的了解请直接阅读沐帅的博士论文和相关发表的论文。</p><p>热备、冷备技术：为了防止 server node 挂掉，导致任务中断，可以采用两个技术，一个是对参数分片进行热备，每个分片存储在三个不同的 server node 中，以 master-slave 的形式存活。如果 master 挂掉，可以快速从 slave 获取并重启相关 task。</p><p>除了热备，还可以定时写入 checkpoint 文件到分布式文件系统来对参数分片及其状态进行备份。进一步保证其安全性。</p><p>Server node 管理：可以使用一致性哈希技术来解决 server node 的加入和退出问题，如图所示：</p><p><img src="snl.png" alt=""></p><p>当有 server node 加入或退出的时候，server manager 负责对参数进行重新分片或者合并。注意在对参数进行分片管理的情况下，一个分片只需要一把锁，这大大提升了系统的性能，也是参数服务器可以实用的一个关键点。</p><h2><span id="4-大规模机器学习的四重境界">4. 大规模机器学习的四重境界</span></h2><p>到这里可以回到我们的标题了，大规模机器学习的四重境界到底是什么呢？</p><p>这四重境界的划分是作者个人阅读总结的一种想法，并不是业界标准，仅供大家参考。</p><p>境界 1：参数可单机存储和更新</p><p>此种境界较为简单，但仍可以使用参数服务器，通过数据并行来加速模型的训练。</p><p>境界 2：参数不可单机存储，可以单机更新</p><p>此种情况对应的是一些简单模型，比如 sparse logistic regression；当 feature 的数量突破百亿的时候，LR 的权重参数不太可能在一台机器上完全存下，此时必须使用参数服务器架构对模型参数进行分片。但是注意一点，SGD 的更新公式：</p><p>(点击放大图像)</p><p>其中可以分开到单个维度进行计算，但是单个维度的 wi=f(w)xi，这里的 f(w) 表示是全部参数 w 的一个函数，具体推导比较简单，这里篇幅所限就不赘述了。只是想说明 worker 在计算梯度的时候可能需要使用到上一轮迭代的所有参数。</p><p>而我们之所以对参数进行分片就是因为我们无法将所有参数存放到一台机器，现在单个 worker 有需要使用所有的参数才能计算某个参数分片的梯度，这不是矛盾吗？可能吗？</p><p>答案是可能的，因为单个样本的 feature 具有很高的稀疏性（sparseness）。例如一个百亿 feature 的模型，单个训练样本往往只在其中很小一部分 feature 上有取值，其他都为 0（假设 feature 取值都已经离散化了）。因此计算 f(w) 的时候可以只拉取不为 0 的 feature 对应的那部分 w 即可。有文章统计，一般这个级别的系统，稀疏性往往在 0.1%（or 0.01%，记得不是很准，大致这样）以下。这样的稀疏性，可以让单机没有任何阻碍的计算 f(w)。</p><p>目前公司开源的 Angel 和 AILab 正在做的系统都处于这个境界。而原生 Spark 还没有达到这个境界，只能在中小规模的圈子里厮混。Angel 改造的基于 Angel 的 Spark 则达到了这个境界。</p><p>境界 3：参数不可单机存储，不可单机更新，但无需模型并行</p><p>境界 3 顺延境界 2 而来，当百亿级 feature 且 feature 比较稠密的时候，就需要计算框架进入到这层境界了，此时单个 worker 的能力有限，无法完整加载一个样本，也无法完整计算 f(w)。怎么办呢？其实很简单，学过线性代数的都知道，矩阵可以分块。向量是最简单的矩阵，自然可以切成一段一段的来计算。只是调度器需要支持算符分段而已了。</p><p>境界 4：参数不可单机存储，不可单机更新，需要模型并行</p><p>进入到这个层次的计算框架，可以算是世界一流了。可以处理超大规模的神经网络。这也是最典型的应用场景。此时不仅模型的参数不能单机存储，而且同一个迭代内，模型参数之间还有强的依赖关系，可以参见姐夫对 distbelief 的介绍里的模型切分。</p><p>此时首先需要增加一个 coordinator 组件来进行模型并行的 concurrent 控制。同时参数服务器框架需要支持 namespace 切分，coordinator 将依赖关系通过 namespace 来进行表示。</p><p>一般参数间的依赖关系因模型而已，所以较难抽象出通用的 coordinator 来，而必须以某种形式通过脚本 parser 来生产整个计算任务的 DAG 图，然后通过 DAG 调度器来完成。对这个问题的介绍可以参考 Erix Xing 的分享 [5]。</p><h2><span id="tensorflow">Tensorflow</span></h2><p>目前业界比较知名的深度学习框架有 Caffee、MXNet、Torch、Keras、Theano 等，但目前最炙手可热的应该是 Google 发布的 Tensorflow。这里单独拿出来稍微分解下。</p><p>前面不少图片引自此文，从 TF 的论文来看，TF 框架本身是支持模型并行和数据并行的，内置了一个参数服务器模块，但从开源版本所曝光的 API 来看，TF 无法用来 10B 级别 feature 的稀疏 LR 模型。原因是已经曝光的 API 只支持在神经网络的不同层和层间进行参数切分，而超大规模 LR 可以看做一个神经单元，TF 不支持单个神经单元参数切分到多个参数服务器 node 上。</p><p>当然，以 Google 的实力，绝对是可以做到第四重境界的，之所以没有曝光，可能是基于其他商业目的的考量，比如使用他们的云计算服务。</p><p>综上，个人认为如果能做到第四重境界，目前可以说的上是世界一流的大规模机器学习框架。仅从沐帅的 ppt 里看他曾经达到过，Google 内部应该也是没有问题的。第三重境界应该是国内一流，第二重应该是国内前列吧。</p><h2><span id="5-其他">5. 其他</span></h2><h3><span id="51-资源管理">5.1 资源管理</span></h3><p>本文没有涉及到的部分是资源管理，大规模机器学习框架部署的集群往往资源消耗也比较大，需要专门的资源管理工具来维护。这方面 Yarn 和 Mesos 都是佼佼者，细节这里也就不介绍了。</p><h3><span id="52-设备">5.2 设备</span></h3><p>除了资源管理工具，本身部署大规模机器学习集群本身对硬件也还是有些要求的，虽然理论上来说，所有 commodity 机器都可以用来搭建这类集群，但是考虑到性能，我们建议尽量用高内存的机器 + 万兆及以上的网卡。没有超快速的网卡，玩参数传递和样本加载估计会比较苦逼。</p><h2><span id="parameter-server介绍">parameter server介绍</span></h2><p>机器学习系统相比于其他系统而言，有一些自己的独特特点。例如：</p><p>迭代性：模型的更新并非一次完成，需要循环迭代多次容错性：即使在每个循环中产生一些错误，模型最终仍能收敛参数收敛的非均匀性：有些参数几轮迭代就会收敛，而有的参数却需要上百轮迭代。而且工业界需要训练大型的机器学习模型，一些广泛应用的特定的模型在规模上有两个特点：</p><p>参数很大，超过单个机器的容纳的能力（大型LR和神经网络）训练数据太大，需要并行提速（大数据）</p><h2><span id="发展历史">发展历史</span></h2><p>参数服务器的概念最早来自于Alex Smola于2010年提出的并行LDA的框架。它通过采用一个分布式的Memcached作为存放参数的存储，这样就提供了有效的机制用于分布式系统中不同的Worker之间同步模型参数，而每个Worker只需要保存他计算时所以来的一小部分参数即可。</p><p>后来由Google的Jeff Dean进一步提出了第一代Google大脑的解决方案：DistBelief。DistBelief将巨大的深度学习模型分布存储在全局的参数服务器中，计算节点通过参数服务器进行信息传递，很好地解决了SGD和L-BFGS算法的分布式训练问题。</p><p>在后来就是李沐所在的DMLC组所设计的参数服务器。根据论文中所写，该parameter server属于第三代参数服务器，就是提供了更加通用的设计。架构上包括一个Server Group和若干个Worker Group。</p><h2><span id="框架介绍">框架介绍</span></h2><p>2.1 框架介绍首先，该PS框架所假设的硬件情况是机器并不可靠，可能在训练中会重启，移动；数据有可能丢失；网络延迟同样也可能很高。因此，在这种情况下，对于那些使用sychronous iterative communication pattern的框架来说，如基于Hadoop的Mahout，就会在训练过程中由于机器的性能表现不均匀而变得非常的慢。</p><p>而对于Parameter Server来说，计算节点被分成两种：worker和servers。workers保留一部分的训练数据，并且执行计算。而servers则共同维持全局共享的模型参数。而worker只和server有通信，互相之间没有通信。</p><p>parameter server具有以下特点：</p><p>Efficient Communication：高效的通信。网络通信开销是机器学习分布式系统中的大头，因此parameter server基本尽了所有的努力来降低网络开销。其中最重要的一点就是：异步通信。因为是异步通信，所以不需要停下来等一些慢的机器执行完一个iter，这就大大减少了延迟。当然并非所有算法都天然的支持异步和随机性，有的算法引入异步后可能收敛会变慢，因此就需要自行在算法收敛和系统效率之间权衡。Elastic Scalability：使用一致性哈希算法，使得新的Server可以随时动态插入集合中，无需重新运行系统Fault Tolerance and Durability：节点故障是不可避免的。对于server节点来说，使用链备份来应对；而对于Worker来说，因为worker之间互相不通信，因此在某个worker失败后，新的worker可以直接加入Ease of Use：全局共享的参数可以被表示成各种形式：vector, matrices或是sparse类型，同时框架还提供对线性代数类型提供高性能的多线程计算库。</p><p><img src="ps_h3.png" alt=""></p><p>Parameter Server框架中，每个server都只负责分到的部分参数（server共同维持一个全局共享参数）。server节点可以和其他server节点通信，每个server负责自己分到的参数，server group共同维持所有参数的更新。server manage node负责维护一些元数据的一致性，例如各个节点的状态，参数的分配情况。</p><p>worker节点之间没有通信，只和对应的server有通信。每个worker group有一个task scheduler，负责向worker分配任务，并且监控worker的运行情况。当有worker退出或加入时，task scheduler重新分配任务。</p><p>##2.2.1 (Key,Value) Vectors参数都被认为是(key, value)集合。例如对于常见的LR来说，key就是feature ID，value就是其权值。对于不存在的key，可认为其权值为0。</p><p>大多数的已有的框架都是这么对(key, value)进行抽象的。但是PS框架除此之外还把这些(k,v)对看作是稀疏的线性代数对象（通过保证key是有序的情况下），因此在对vector进行计算操作的时候，也会在某些操作上使用BLAS库等线性代数库进行优化。</p><h2><span id="222-range-pushpull">2.2.2 Range Push/Pull</span></h2><p>PS框架中，workers和servers之间通信是通过 push() 和 pull() 方法进行的。worker通过push()将计算好的梯度发送到server，然后通过pull()从server获取更新参数。</p><p>为了提高通信效率，PS允许用户使用Range Push/Range Pull操作。</p><p>w.push(R, dest)w.pull(R, dest)</p><h2><span id="223-asynchronous-tasks-and-dependency">2.2.3 Asynchronous Tasks and Dependency</span></h2><p>task即为一个RPC调用函数。举例而言，worker对server的RPC调用push()或pull()都是算作一个task，又或是scheduler中用户自定义的一个用来对任意其他节点进行RPC调用的函数都算做是task。</p><p>而异步指的是，Tasks在被caller调用后立刻返回，但只有当caller收到callee的回复后，才将该task标记为结束。（在push()和pull()的例子中，worker==caller调用者，server==callee被调用者）默认而言，callee为了性能执行tasks都是并行的，但是如果caller希望串行执行task的话，可以在不同的task之间添加execute-after-finished依赖。</p><p><img src="dsd.png" alt=""></p><p>上图中，Task Scheduler的 调用了Worker中的WORKERITERATE()，这算做是一个task。其中Task Scheduler是caller，worker是callee。</p><p>而WORKERITERATE()这个task却包括3步，第一步是计算梯度，因为是本地的，所以不算task。但是后续的push()和pull()都是subtask。</p><p>假设，Scheduler要求iter10和iter11是独立的，但是iter11和iter12是添加了依赖的。那么对于其中某个worker来说，他的执行过程就类似于下图<img src="dsd1.png" alt=""></p><p>该worker在iter10的梯度计算后就立刻计算iter11的梯度，然而在iter11的梯度计算完后，却等待push()+pull()两个subtask结束才开始iter12的计算。</p><h2><span id="224-flexible-consistency">2.2.4 Flexible Consistency</span></h2><p>异步task能够提高系统的效率，但是却会造成数据不一致，从而降低算法的收敛速率。</p><p>上例中，该worker在iter10的结果参数被pull()之前就开始计算iter11的梯度，因此他使用的模型参数仍然是，所以iter11计算得到的梯度和iter10相同，这就拖慢了算法的收敛速率。</p><p>但有些算法对于数据的不一致性不那么敏感，因此这需要开发者自行权衡系统性能和算法收敛速率，这就需要考虑：</p><p>算法对于参数非一致性的敏感度训练数据特征之间的关联度硬盘的存储容量</p><h2><span id="225-user-defined-filters">2.2.5 User-defined Filters</span></h2><p>PS所支持的另外一个减少网络带宽的方法是支持用户自定义过滤器来过滤掉那些比较小的被push的entry。</p><p>常用的过滤器有significantly modified filter，即只push大于某一门槛的entry。也有KKT filter，利用最优化问题的一些条件过滤掉对weights影响不大的entry。</p><h2><span id="23-parameter-server的异步性与非凸问题">2.3 parameter server的异步性与非凸问题</span></h2><p>参数服务器模型更新的时候，worker的模型参数与server的模型参数可能有所不一致。</p><p>举例而言，梯度计算需要基于某个特定的参数值（相当于下山，我们只能找到针对特定某点的下山最快的方向，一旦该点变化，则下山最快的方向也就不对了）。问题在于：节点A从server获得参数值后，当计算完梯度后，此时server端的参数值可能已经被节点B所更新了。</p><p>但是在非凸问题（例如深度学习的优化）中，这反而是个好事，引入了随机性。这是因为非凸问题本身就不是梯度下降能够解决的，正常的单机迭代肯定会收敛到局部最优。有时我们常常会用一些额外的方法来跳出局部最优：</p><p>多组参数值初始化模拟退火随机梯度下降而上面所说的PS框架正好利用异步性引入了随机性，有助于跳出局部最优。因此在Google的DistBelief框架中，提出了Downpour SGD算法，就是尽最大可能利用了这种随机性。</p><h2><span id="241-vector-clock">2.4.1 Vector Clock</span></h2><p>PS使用vector clock来记录每个节点的参数，用来跟踪数据状态或避免数据重复发送。但假设有n个节点，m个参数，那么vector clock的空间复杂度就是O(nm)，无法承受。</p><p>幸运的是，parameter server在push和pull的时候，都是range-based，因此这个range里参数共享的是同一个时间戳，这就降低了复杂度（具体实现见下面ps-lite源码剖析）。</p><h2><span id="242-messages">2.4.2 Messages</span></h2><p>一条message包括时间戳，和(k,v)对。但是由于机器学习问题频繁的参数访问，导致信息的压缩是必然的。有两种优化来压缩网络带宽：</p><p>key的压缩：因为训练数据在分配之后通常不会改变，因此worker没必要每次都发送相同的key，只需要在接受方第一次接收时缓存起来即可。后续只需发送value用户自定义的过滤器：有些参数更新并非对最终优化有价值，因此用户可以自定义过滤规则来过滤掉一些不必要的传送。例如对于梯度下降来说，很小的梯度值是低效的，可以忽略；同样当更新接近最优的时候，值也是低效的，可以忽略。（这个通过KKT条件来判断）2.4.3 Replication and ConsistencyPS在数据一致性方面，使用的是一致性哈希算法，然后每个节点备份其逆时针的k个节点的参数。</p><p>一致性哈希算法：即将数据按照某种hash算法映射到环上，然后将机器按照同样的hash算法映射到环上，将数据存储到环上顺时针最近的机器上。</p><p><img src="css.png" alt=""></p><p>如图，k=3，则S2，S3和S4复制了S1的参数。</p><h1><span id="第二章-参数服务器设计理念与基本架构">第二章 参数服务器设计理念与基本架构</span></h1><h1><span id="第三章-pslite的分析">第三章 pslite的分析</span></h1><h1><span id="第四章-tensorflow-ps分析">第四章 tensorflow ps分析</span></h1><h1><span id="第五章-angel-ps分析">第五章 angel ps分析</span></h1><h1><span id="第六章-实现参数服务器">第六章 实现参数服务器</span></h1><h1><span id="第七章-调度框架的简要分析">第七章 调度框架的简要分析</span></h1><h1><span id="第八章-对参数服务器的思考和展望">第八章 对参数服务器的思考和展望</span></h1><h1><span id="引用">引用</span></h1><ol><li>Wide &amp; Deep Learning for Recommender Systems</li><li>Deep Neural Networks for YouTube Recommendations</li><li>https://www.zhihu.com/question/53851014</li><li>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems</li><li>http://www.jianshu.com/p/00736aa21dc8</li><li>Large Scale Distributed Deep Networks</li><li>MapReduce: Simplified Data Processing on Large Clusters</li><li>Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</li><li>https://www.zhihu.com/question/55119470</li><li>KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba and Ant Financial</li><li>An Architecture for Parallel Topic Models</li><li>Scaling Distributed Machine Learning with the Parameter Server</li><li>Piccolo: Building fast, distributed pro- grams with partitioned tables</li><li>More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server</li><li>Angel-A Flexible and Powerful Parameter Server；黄明 ppt</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;parameter-server技术内幕&quot;&gt;parameter server技术内幕&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;深入解析parameter server架构设计与实现原理&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;第一章-回顾分
      
    
    </summary>
    
    
      <category term="参数服务器" scheme="http://www.yifanguo.top/tags/%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    
  </entry>
  
  <entry>
    <title>在线最优化求解</title>
    <link href="http://www.yifanguo.top/2018/10/23/%E5%9C%A8%E7%BA%BF%E6%9C%80%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3/"/>
    <id>http://www.yifanguo.top/2018/10/23/在线最优化求解/</id>
    <published>2018-10-22T18:09:32.000Z</published>
    <updated>2018-10-24T02:00:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>SGD或L-BFGS</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SGD或L-BFGS&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="online learning" scheme="http://www.yifanguo.top/tags/online-learning/"/>
    
  </entry>
  
  <entry>
    <title>ftrl</title>
    <link href="http://www.yifanguo.top/2018/10/23/ftrl/"/>
    <id>http://www.yifanguo.top/2018/10/23/ftrl/</id>
    <published>2018-10-22T18:01:48.000Z</published>
    <updated>2018-10-23T09:08:51.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="ftrl-follow-the-regularized-leader">FTRL -- Follow the Regularized Leader</span></h1><h1><span id="背景">背景</span></h1><p>在工业界，越来越多的业务需要大规模机器学习，不单参与训练的数据量大，模型特征量的规模也大。例如点击率预估，训练数据量在TB量级，特征量在亿这个量级，业内常用LR（Logistic Regression）和FM（Factorization Machines）为点击率预估建模。对LR、FM这类模型的参数学习，传统的学习算法是batch learning算法，它无法有效地处理大规模的数据集，也无法有效地处理大规模的在线数据流。这时，有效且高效的online learning算法显得尤为重要。</p><pre><code>   SGD算法[1]是常用的online learning算法，它能学习出不错的模型，但学出的模型不是稀疏的。为此，学术界和工业界都在研究这样一种online learning算法，它能学习出有效的且稀疏的模型。FTRL（Follow the Regularized Leader）算法正是这样一种算法，它由Google的H. Brendan McMahan在2010年提出的[2]，后来在2011年发表了一篇关于FTRL和AOGD、FOBOS、RDA比较的论文[3]，2013年又和Gary Holt, D. Sculley, Michael Young等人发表了一篇关于FTRL工程化实现的论文[4]。如论文[4]的内容所述，FTRL算法融合了RDA算法能产生稀疏模型的特性和SGD算法能产生更有效模型的特性。它在处理诸如LR之类的带非光滑正则化项（例如1范数，做模型复杂度控制和稀疏化）的凸优化问题上性能非常出色，国内各大互联网公司都已将该算法应用到实际产品中。   文章[5]是篇很好的介绍FTRL算法的中文资料，从TG算法、FOBOS算法开始，到RDA算法，最后到FTRL算法，一脉相承，而且各个算法都有推导过程，值得认真体会。   这篇文章不会赘述文章[5]中的内容，而是介绍FTRL算法与SGD算法之间存在的另一种联系。这个联系在网络上似乎没有文章介绍，可能是因为这个细节不那么重要，但倘若了解了此细节，能更好的体会到FTRL算法为啥跟SGD算法有联系。</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;ftrl-follow-the-regularized-leader&quot;&gt;FTRL -- Follow the Regularized Leader&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;背景&quot;&gt;背景&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;在工业界
      
    
    </summary>
    
    
      <category term="ftrl" scheme="http://www.yifanguo.top/tags/ftrl/"/>
    
  </entry>
  
  <entry>
    <title>socket</title>
    <link href="http://www.yifanguo.top/2018/10/22/socket/"/>
    <id>http://www.yifanguo.top/2018/10/22/socket/</id>
    <published>2018-10-21T20:33:40.000Z</published>
    <updated>2018-10-22T11:50:57.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="socket是什么">socket是什么</span></h1><p><img src="socket.png" alt=""></p><p>Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议</p><p><img src="server.png" alt=""></p><p>先从服务器端说起。服务器端先初始化Socket，然后与端口绑定(bind)，对端口进行监听(listen)，调用accept阻塞，等待客户端连接。在这时如果有个客户端初始化一个Socket，然后连接服务器(connect)，如果连接成功，这时客户端与服务器端的连接就建立了。客户端发送数据请求，服务器端接收请求并处理请求，然后把回应数据发送给客户端，客户端读取数据，最后关闭连接，一次交互结束。</p><h1><span id="网络中进程之间如何通信">网络中进程之间如何通信</span></h1><p>本地的进程间通信（IPC）有很多种方式，但可以总结为下面4类：</p><ul><li>消息传递（管道、FIFO、消息队列）</li><li>同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量）</li><li>共享内存（匿名的和具名的）</li><li>远程过程调用（Solaris门和Sun RPC）</li></ul><p>但这些都不是本文的主题！我们要讨论的是网络中进程之间如何通信？首要解决的问题是如何唯一标识一个进程，否则通信无从谈起！在本地可以通过进程PID来唯一标识一个进程，但是在网络中这是行不通的。其实TCP/IP协议族已经帮我们解决了这个问题，网络层的“ip地址”可以唯一标识网络中的主机，而传输层的“协议+端口”可以唯一标识主机中的应用程序（进程）。这样利用三元组（ip地址，协议，端口）就可以标识网络的进程了，网络中的进程通信就可以利用这个标志与其它进程进行交互。</p><p>使用TCP/IP协议的应用程序通常采用应用编程接口：UNIX BSD的套接字（socket）和UNIX System V的TLI（已经被淘汰），来实现网络进程之间的通信。就目前而言，几乎所有的应用程序都是采用socket，而现在又是网络时代，网络中进程通信是无处不在，这就是我为什么说“一切皆socket”</p><h1><span id="什么是socket">什么是socket</span></h1><p>上面我们已经知道网络中的进程是通过socket来通信的，那什么是socket呢？socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，都可以用“打开open –&gt; 读写write/read –&gt; 关闭close”模式来操作。我的理解就是Socket就是该模式的一个实现，socket即是一种特殊的文件，一些socket函数就是对其进行的操作（读/写IO、打开、关闭</p><h1><span id="socket的基本操作">socket的基本操作</span></h1><p>既然socket是“open—write/read—close”模式的一种实现，那么socket就提供了这些操作对应的函数接口。下面以TCP为例，介绍几个基本的socket接口函数</p><h2><span id="31-socket函数">3.1 socket()函数</span></h2><p>int socket(int domain, int type, int protocol);</p><p>socket函数对应于普通文件的打开操作。普通文件的打开操作返回一个文件描述字，而socket()用于创建一个socket描述符（socket descriptor），它唯一标识一个socket。这个socket描述字跟文件描述字一样，后续的操作都有用到它，把它作为参数，通过它来进行一些读写操作。</p><p>正如可以给fopen的传入不同参数值，以打开不同的文件。创建socket的时候，也可以指定不同的参数创建不同的socket描述符，socket函数的三个参数分别为：</p><ul><li>domain：即协议域，又称为协议族（family）。常用的协议族有，AF_INET、AF_INET6、AF_LOCAL（或称AF_UNIX，Unix域socket）、AF_ROUTE等等。协议族决定了socket的地址类型，在通信中必须采用对应的地址，如AF_INET决定了要用ipv4地址（32位的）与端口号（16位的）的组合、AF_UNIX决定了要用一个绝对路径名作为地址。</li><li>type：指定socket类型。常用的socket类型有，SOCK_STREAM、SOCK_DGRAM、SOCK_RAW、SOCK_PACKET、SOCK_SEQPACKET等等（socket的类型有哪些？）。</li><li>protocol：故名思意，就是指定协议。常用的协议有，IPPROTO_TCP、IPPTOTO_UDP、IPPROTO_SCTP、IPPROTO_TIPC等，它们分别对应TCP传输协议、UDP传输协议、STCP传输协议、TIPC传输协议</li></ul><p>注意：并不是上面的type和protocol可以随意组合的，如SOCK_STREAM不可以跟IPPROTO_UDP组合。当protocol为0时，会自动选择type类型对应的默认协议。</p><p>当我们调用socket创建一个socket时，返回的socket描述字它存在于协议族（address family，AF_XXX）空间中，但没有一个具体的地址。如果想要给它赋值一个地址，就必须调用bind()函数，否则就当调用connect()、listen()时系统会自动随机分配一个端口</p><h2><span id="32-bind函数">3.2 bind()函数</span></h2><p>int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);</p><p>通常服务器在启动的时候都会绑定一个众所周知的地址（如ip地址+端口号），用于提供服务，客户就可以通过它来接连服务器；而客户端就不用指定，有系统自动分配一个端口号和自身的ip地址组合。这就是为什么通常服务器端在listen之前会调用bind()，而客户端就不会调用，而是在connect()时由系统随机生成一个</p><h2><span id="33-listen-connect函数">3.3 listen()、connect()函数</span></h2><p>如果作为一个服务器，在调用socket()、bind()之后就会调用listen()来监听这个socket，如果客户端这时调用connect()发出连接请求，服务器端就会接收到这个请求。int listen(int sockfd, int backlog);int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);</p><p>listen函数的第一个参数即为要监听的socket描述字，第二个参数为相应socket可以排队的最大连接个数。socket()函数创建的socket默认是一个主动类型的，listen函数将socket变为被动类型的，等待客户的连接请求。</p><p>connect函数的第一个参数即为客户端的socket描述字，第二参数为服务器的socket地址，第三个参数为socket地址的长度。客户端通过调用connect函数来建立与TCP服务器的连接。</p><h2><span id="34-accept函数">3.4 accept()函数</span></h2><p>TCP服务器端依次调用socket()、bind()、listen()之后，就会监听指定的socket地址了。TCP客户端依次调用socket()、connect()之后就想TCP服务器发送了一个连接请求。TCP服务器监听到这个请求之后，就会调用accept()函数取接收请求，这样连接就建立好了。之后就可以开始网络I/O操作了，即类同于普通文件的读写I/O操作。</p><p>int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);accept函数的第一个参数为服务器的socket描述字，第二个参数为指向struct sockaddr *的指针，用于返回客户端的协议地址，第三个参数为协议地址的长度。如果accpet成功，那么其返回值是由内核自动生成的一个全新的描述字，代表与返回客户的TCP连接。</p><p>注意：accept的第一个参数为服务器的socket描述字，是服务器开始调用socket()函数生成的，称为监听socket描述字；而accept函数返回的是已连接的socket描述字。一个服务器通常通常仅仅只创建一个监听socket描述字，它在该服务器的生命周期内一直存在。内核为每个由服务器进程接受的客户连接创建了一个已连接socket描述字，当服务器完成了对某个客户的服务，相应的已连接socket描述字就被关闭。</p><h1><span id="35-read-write等函数">3.5 read()、write()等函数</span></h1><p>read函数是负责从fd中读取内容.当读成功时，read返回实际所读的字节数，如果返回的值是0表示已经读到文件的结束了，小于0表示出现了错误。如果错误为EINTR说明读是由中断引起的，如果是ECONNREST表示网络连接出了问题。</p><p>write函数将buf中的nbytes字节内容写入文件描述符fd.成功时返回写的字节 数。失败时返回-1，并设置errno变量。在网络程序中，当我们向套接字文件描述符写时有俩种可能。1)write的返回值大于0，表示写了部分或者是 全部的数据。2)返回的值小于0，此时出现了错误。我们要根据错误类型来处理。如果错误为EINTR表示在写的时候出现了中断错误。如果为EPIPE表示 网络连接出现了问题(对方已经关闭了连接)。</p><p>其它的我就不一一介绍这几对I/O函数了，具体参见man文档或者baidu、Google，下面的例子中将使用到send/recv。</p><h1><span id="36-close函数">3.6 close()函数</span></h1><p>在服务器与客户端建立连接之后，会进行一些读写操作，完成了读写操作就要关闭相应的socket描述字，好比操作完打开的文件要调用fclose关闭打开的文件。</p><p>#includeint close(int fd);close一个TCP socket的缺省行为时把该socket标记为以关闭，然后立即返回到调用进程。该描述字不能再由调用进程使用，也就是说不能再作为read或write的第一个参数。</p><p>注意：close操作只是使相应socket描述字的引用计数-1，只有当引用计数为0的时候，才会触发TCP客户端向服务器发送终止连接请求。</p><h1><span id="4-socket中tcp的三次握手建立连接详解">4、socket中TCP的三次握手建立连接详解</span></h1><p>我们知道tcp建立连接要进行“三次握手”，即交换三个分组。大致流程如下：</p><p>客户端向服务器发送一个SYN J服务器向客户端响应一个SYN K，并对SYN J进行确认ACK J+1客户端再想服务器发一个确认ACK K+1只有就完了三次握手，但是这个三次握手发生在socket的那几个函数中呢？请看下图：</p><p><img src="shakehands.png" alt=""></p><p>从图中可以看出，当客户端调用connect时，触发了连接请求，向服务器发送了SYN J包，这时connect进入阻塞状态；服务器监听到连接请求，即收到SYN J包，调用accept函数接收请求向客户端发送SYN K ，ACK J+1，这时accept进入阻塞状态；客户端收到服务器的SYN K ，ACK J+1之后，这时connect返回，并对SYN K进行确认；服务器收到ACK K+1时，accept返回，至此三次握手完毕，连接建立。</p><p>总结：客户端的connect在三次握手的第二个次返回，而服务器端的accept在三次握手的第三次返回。</p><h1><span id="5-socket中tcp的四次握手释放连接详解">5、socket中TCP的四次握手释放连接详解</span></h1><p>上面介绍了socket中TCP的三次握手建立过程，及其涉及的socket函数。现在我们介绍socket中的四次握手释放连接的过程，请看下图：</p><p><img src="offhands.png" alt=""></p><p>图示过程如下：</p><ul><li>某个应用进程首先调用close主动关闭连接，这时TCP发送一个FIN M；</li><li>另一端接收到FIN M之后，执行被动关闭，对这个FIN进行确认。它的接收也作为文件结束符传递给应用进程，因为FIN的接收意味着应用进程在相应的连接上再也接收不到额外数据；</li><li>一段时间之后，接收到文件结束符的应用进程调用close关闭它的socket。这导致它的TCP也发送一个FIN N；</li><li>接收到这个FIN的源发送端TCP对它进行确认。这样每个方向上都有一个FIN和ACK。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;socket是什么&quot;&gt;socket是什么&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;socket.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实
      
    
    </summary>
    
    
      <category term="socket" scheme="http://www.yifanguo.top/tags/socket/"/>
    
  </entry>
  
  <entry>
    <title>xgboost</title>
    <link href="http://www.yifanguo.top/2018/10/17/xgboost/"/>
    <id>http://www.yifanguo.top/2018/10/17/xgboost/</id>
    <published>2018-10-17T11:41:44.000Z</published>
    <updated>2018-10-19T02:10:13.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="xgboost" scheme="http://www.yifanguo.top/tags/xgboost/"/>
    
  </entry>
  
  <entry>
    <title>lightgbm</title>
    <link href="http://www.yifanguo.top/2018/10/17/lightgbm-0/"/>
    <id>http://www.yifanguo.top/2018/10/17/lightgbm-0/</id>
    <published>2018-10-17T10:49:34.000Z</published>
    <updated>2018-10-18T02:42:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="background">background</span></h1><p>In recent years, with the emergence of big data (in terms of both the number of features and the number of instances), GBDT is facing new challenges, especially in the tradeoff between accuracy and efficiency.</p><h1><span id="why-introduct-lightgbm">why introduct lightgbm</span></h1><p>A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming.</p><h1><span id="what-is-lightgbm">what is lightgbm</span></h1><p>GOSS: Gradient-based One-Side SamplingEFB: Exclusive Feature Bundling</p><h1><span id="goss">GOSS</span></h1><p>ince the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.</p><h1><span id="efb">EFB</span></h1><p>With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features.</p><h1><span id="gbdt">GBDT</span></h1><p>GBDT is an ensemble model of decision trees,In each iteration, GBDT learns the decision trees by fitting the negative gradients (also known as residual errors).</p><p>The main cost in GBDT lies in learning the decision trees, and the most time-consuming part in learning a decision tree is to find the best split points</p><h1><span id="goss-implementaion">GOSS implementaion</span></h1><p>GOSS keeps all the instances with large gradients and performs random sampling on the instances with small gradients.</p><p><img src="/Users/yifanguo/Desktop/xgb.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;background&quot;&gt;background&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;In recent years, with the emergence of big data (in terms of both the number of features
      
    
    </summary>
    
    
      <category term="lightgbm" scheme="http://www.yifanguo.top/tags/lightgbm/"/>
    
  </entry>
  
  <entry>
    <title>lightGBM on spark</title>
    <link href="http://www.yifanguo.top/2018/10/08/mmlSpark/"/>
    <id>http://www.yifanguo.top/2018/10/08/mmlSpark/</id>
    <published>2018-10-07T18:28:18.000Z</published>
    <updated>2018-10-12T07:44:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="lightgbm-on-spark-参数配置">LightGBM on Spark 参数配置</span></h1><h1><span id="官方参数说明">官方参数说明</span></h1><p>http://lightgbm.apachecn.org/cn/latest/Parameters.html</p><h1><span id="工程地址">工程地址</span></h1><p>https://g.hz.netease.com/guoyifan01/lightgbmOnSpark</p><h1><span id="configuration">configuration</span></h1><h2><span id="通用">通用</span></h2><table><thead><tr><th>parameter</th><th>lightgbm param</th><th>default</th><th>type</th><th>description</th></tr></thead><tbody><tr><td>prePartition</td><td>is_pre_partition</td><td>True</td><td>bool</td><td>true 如果训练数据 pre-partitioned, 不同的机器使用不同的分区</td></tr><tr><td>boostingType</td><td>boosting_type</td><td>gbdt</td><td>string</td><td>gbdt,rf, dart, goss</td></tr><tr><td>parallelism</td><td>tree_learner</td><td>data_parallel</td><td>string</td><td>数据并行的 tree learner</td></tr><tr><td>defaultListenPort</td><td>无</td><td>12400</td><td>int</td><td>The default listen port on executors, used for testing</td></tr><tr><td>numIterations</td><td>num_iterations</td><td>100</td><td>int</td><td>boosting 的迭代次数</td></tr><tr><td>learningRate</td><td>learning_rate</td><td>0.1</td><td>double</td><td>学习率</td></tr><tr><td>numLeaves</td><td>num_leaves</td><td>31</td><td>int</td><td>一棵树上的叶子数</td></tr><tr><td>maxBin</td><td>max_bin</td><td>255</td><td>int</td><td>工具箱的最大数特征值决定了容量 工具箱的最小数特征值可能会降低训练的准确性, 但是可能会增加一些一般的影响（处理过度学习）LightGBM 将根据 max_bin 自动压缩内存。 例如, 如果 maxbin=255, 那么 LightGBM 将使用 uint8t 的特性值</td></tr><tr><td>baggingFraction</td><td>bagging_fraction</td><td>1.0</td><td>double</td><td>类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据 可以用来加速训练 可以用来处理过拟合 Note: 为了启用 bagging, bagging_freq 应该设置为非零值</td></tr><tr><td>baggingFreq</td><td>bagging_freq</td><td>0</td><td>int</td><td>bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging Note: 为了启用 bagging, bagging_fraction 设置适当</td></tr><tr><td>baggingSeed</td><td>bagging_seed</td><td>3</td><td>int</td><td>Bagging seed</td></tr><tr><td>earlyStoppingRound</td><td>early_stopping_round</td><td>0</td><td>int</td><td>Early stopping round</td></tr><tr><td>featureFraction</td><td>feature_fraction</td><td>1.0</td><td>double</td><td>Feature fraction</td></tr><tr><td>maxDepth</td><td>max_depth</td><td>-1</td><td>int</td><td>-1意味着没有限制</td></tr><tr><td>minSumHessianInLeaf</td><td>min_sum_hessian_in_leaf</td><td>1e-3</td><td>Double</td><td>Minimal sum hessian in one leaf</td></tr><tr><td>无</td><td>num_machines</td><td>1</td><td>int</td><td>并行运行的机器数量，在spark上，有多少个executor就有多少个机器</td></tr><tr><td>timeout</td><td>无</td><td>120s</td><td>second</td><td>socket超时</td></tr><tr><td>objective</td><td>application</td><td>regression</td><td>enum</td><td>regression, regression_l1, huber, fair, poisson, quantile, quantile_l2, binary, multiclass, multiclassova, xentropy, xentlambda, lambdarank</td></tr><tr><td>min_data_in_leaf</td><td>min_data_in_leaf</td><td>20</td><td>int</td><td>min_data_in_leaf</td></tr><tr><td>min_data_in_bin</td><td>min_data_in_bin</td><td>3</td><td>int</td><td>min_data_in_bin</td></tr><tr><td>lambda_l1</td><td>lambda_l1</td><td>0</td><td>double</td><td>lambda_l1</td></tr><tr><td>lambda_l2</td><td>lambda_l2</td><td>0</td><td>double</td><td>lambda_l2</td></tr><tr><td>modelString</td><td>无</td><td>“”</td><td>String</td><td>model存成String格式</td></tr></tbody></table><p>##分类</p><table><thead><tr><th>parameter</th><th>lightgbm param</th><th>default</th><th>type</th><th>description</th></tr></thead><tbody><tr><td>无</td><td>metric</td><td>binary_logloss,auc</td><td>String</td><td>度量</td></tr></tbody></table><p>##回归</p><table><thead><tr><th>parameter</th><th>lightgbm param</th><th>default</th><th>type</th><th>description</th></tr></thead><tbody><tr><td>alpha</td><td>alpha</td><td>0.9</td><td>double</td><td>Huber loss 和 Quantile regression 的参数. 将用于 regression 任务</td></tr><tr><td>tweedieVariancePower</td><td>tweedie_variance_power</td><td>1.5</td><td>double</td><td>control the variance of tweedie distribution, must be between 1 and 2</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;lightgbm-on-spark-参数配置&quot;&gt;LightGBM on Spark 参数配置&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;官方参数说明&quot;&gt;官方参数说明&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;http://lig
      
    
    </summary>
    
    
      <category term="ml" scheme="http://www.yifanguo.top/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>LightGBM</title>
    <link href="http://www.yifanguo.top/2018/10/08/LightGBM/"/>
    <id>http://www.yifanguo.top/2018/10/08/LightGBM/</id>
    <published>2018-10-07T17:07:39.000Z</published>
    <updated>2018-10-08T08:08:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="light-gbm">light gbm</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;light-gbm&quot;&gt;light gbm&lt;/span&gt;&lt;/h1&gt;

      
    
    </summary>
    
    
      <category term="ml" scheme="http://www.yifanguo.top/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title>Scala的一些解释</title>
    <link href="http://www.yifanguo.top/2018/10/07/Scala/"/>
    <id>http://www.yifanguo.top/2018/10/07/Scala/</id>
    <published>2018-10-07T14:35:26.000Z</published>
    <updated>2018-10-08T08:07:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="with-vs-extends">with vs extends</span></h1><p>https://stackoverflow.com/questions/41031166/scala-extends-vs-with</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;with-vs-extends&quot;&gt;with vs extends&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;https://stackoverflow.com/questions/41031166/scala-extends-vs-with&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="scala" scheme="http://www.yifanguo.top/tags/scala/"/>
    
  </entry>
  
  <entry>
    <title>GBDT</title>
    <link href="http://www.yifanguo.top/2018/10/07/GBDT/"/>
    <id>http://www.yifanguo.top/2018/10/07/GBDT/</id>
    <published>2018-10-07T10:43:15.000Z</published>
    <updated>2018-10-08T04:34:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="reference">Reference：</span></h1><p>https://www.cnblogs.com/ModifyRong/p/7744987.html</p><h1><span id="residual-残差">residual 残差</span></h1><h1><span id="gbdt-gradient-boosting-decision-tree">GBDT - Gradient Boosting Decision Tree</span></h1><h2><span id="dt-决策树">DT-- 决策树</span></h2><p>特点： 可以做分类和回归，分类速度快，可以可视化缺点：容易过拟合</p><h2><span id="boosting">Boosting</span></h2><p>在分类问题中，它通过改变训练样本的权重（增加分错样本的权重，减小分队样本的的权重），学习多个分类器，并将这些分类器线性组合，提高分类器性能。</p><p>Boosting 是一族可将弱学习器提升为强学习器的算法，属于集成学习（ensemble learning）的范畴。Boosting 方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断要好。通俗地说，就是&quot;三个臭皮匠顶个诸葛亮&quot;的道理。</p><p>基于梯度提升算法的学习器叫做 GBM(Gradient Boosting Machine)。理论上，GBM 可以选择各种不同的学习算法作为基学习器。GBDT 实际上是 GBM 的一种情况。</p><h2><span id="gradient-boosting">Gradient Boosting</span></h2><p>Gradient Boosting是一种Boosting方法，主要思想是：每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能(一般为拟合程度+正则项)，认为损失函数越小，性能越好。让损失函数持续下降，就能使模型不断改进提升性能，最好的方法就是使损失函数沿着梯度方向下降。</p><h1><span id="gbdt-introduction">GBDT introduction</span></h1><p><img src="gb.png" alt=""></p><p>gbdt训练过程</p><p>gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度</p><p>弱分类器一般会选择为CART TREE（也就是分类回归树）</p><p>$$$$</p><p>Fm−1(x) 为当前的模型，gbdt 通过经验风险极小化来确定下一个弱分类器的参数。具体到损失函数本身的选择也就是L的选择，有平方损失函数，0-1损失函数，对数损失函数等等。如果我们选择平方损失函数，那么这个差值其实就是我们平常所说的残差</p><h1><span id="gbdt-key-points">GBDT key points</span></h1><ul><li>希望loss_f 能够不断减小，沿着剃度方向</li><li>希望loss_f 能够尽快的减小</li></ul><p>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值去拟合一个回归树。gbdt 每轮迭代的时候，都去拟合损失函数在当前模型下的负梯度。这样每轮训练的时候都能够让损失函数尽可能快的减小，尽快的收敛达到局部最优解或者全局最优解</p><h1><span id="gbdt如何选择特征">GBDT如何选择特征</span></h1><p>gbdt的弱分类器默认选择的是CART TREE。其实也可以选择其他弱分类器的，选择的前提是低方差和高偏差。框架服从boosting 框架即可</p><p>下面我们具体来说CART TREE(是一种二叉树) 如何生成。CART TREE 生成的过程其实就是一个选择特征的过程。假设我们目前总共有 M 个特征。第一步我们需要从中选择出一个特征 j，做为二叉树的第一个节点。然后对特征 j 的值选择一个切分点 m. 一个 样本的特征j的值 如果小于m，则分为一类，如果大于m,则分为另外一类。如此便构建了CART 树的一个节点。其他节点的生成过程和这个是一样的。现在的问题是在每轮迭代的时候，如何选择这个特征 j,以及如何选择特征 j 的切分点 m:</p><ul><li>原始的gbdt的做法非常的暴力，首先遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征 m 的最优切分点 j。</li><li>如何衡量我们找到的特征 m和切分点 j 是最优的呢？ 我们用定义一个函数 FindLossAndSplit 来展示一下求解过程：</li></ul><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def findLossAndSplit(x,y):</span><br><span class="line">    # 我们用 x 来表示训练数据</span><br><span class="line">    # 我们用 y 来表示训练数据的label</span><br><span class="line">    # x[i]表示训练数据的第i个特征</span><br><span class="line">    # x_i 表示第i个训练样本</span><br><span class="line"></span><br><span class="line">    # minLoss 表示最小的损失</span><br><span class="line">    minLoss = Integet.max_value</span><br><span class="line">    # feature 表示是训练的数据第几纬度的特征</span><br><span class="line">    feature = 0</span><br><span class="line">    # split 表示切分点的个数</span><br><span class="line">    split = 0</span><br><span class="line"></span><br><span class="line">    # M 表示 样本x的特征个数</span><br><span class="line">    for j in range(0,M):</span><br><span class="line">        # 该维特征下，特征值的每个切分点，这里具体的切分方式可以自己定义</span><br><span class="line">        for c in range(0,x[j]):</span><br><span class="line">            L = 0</span><br><span class="line">            # 第一类</span><br><span class="line">            R1 = &#123;x|x[j] &lt;= c&#125;</span><br><span class="line">            # 第二类</span><br><span class="line">            R2 = &#123;x|x[j] &gt; c&#125;</span><br><span class="line">            # 属于第一类样本的y值的平均值</span><br><span class="line">            y1 = ave&#123;y|x 属于 R1&#125;</span><br><span class="line">            # 属于第二类样本的y值的平均值</span><br><span class="line">            y2 = ave&#123;y| x 属于 R2&#125;</span><br><span class="line">            # 遍历所有的样本，找到 loss funtion 的值</span><br><span class="line">            for x_1 in all x</span><br><span class="line">                if x_1 属于 R1： </span><br><span class="line">                    L += (y_1 - y1)^2 </span><br><span class="line">                else:</span><br><span class="line">                    L += (y_1 - y2)^2</span><br><span class="line">            if L &lt; minLoss:</span><br><span class="line">               minLoss = L</span><br><span class="line">               feature  = i</span><br><span class="line">               split = c</span><br><span class="line">    return minLoss,feature ,split</span><br></pre></td></tr></table></figure></p><h1><span id="gbdt如何构建特征">gbdt如何构建特征</span></h1><p><img src="gbdt_lr.png" alt=""></p><p>迭代几次，就会产生几棵树</p><p>我们 使用 GBDT 生成了两棵树，两颗树一共有五个叶子节点。我们将样本 X 输入到两颗树当中去，样本X 落在了第一棵树的第二个叶子节点，第二颗树的第一个叶子节点，于是我们便可以依次构建一个五纬的特征向量，每一个纬度代表了一个叶子节点，样本落在这个叶子节点上面的话那么值为1，没有落在该叶子节点的话，那么值为 0.</p><p>于是对于该样本，我们可以得到一个向量[0,1,0,1,0] 作为该样本的组合特征，和原来的特征一起输入到逻辑回归当中进行训练。实验证明这样会得到比较显著的效果提升。</p><h1><span id="gbdt-vs-xgboost">GBDT vs xGboost</span></h1><p>Xgboost 和 GBDT 的区别：</p><p>GBDT：</p><p>GBDT 它的非线性变换比较多，表达能力强，而且不需要做复杂的特征工程和特征变换。GBDT 的缺点也很明显，Boost 是一个串行过程，不好并行化，而且计算复杂度高，同时不太适合高维稀疏特征；传统 GBDT 在优化时只用到一阶导数信息。Xgboost：</p><p>它有以下几个优良的特性：</p><ol><li>显示的把树模型复杂度作为正则项加到优化目标中。</li><li>公式推导中用到了二阶导数，用了二阶泰勒展开。（GBDT 用牛顿法貌似也是二阶信息）</li><li>实现了分裂点寻找近似算法。</li><li>利用了特征的稀疏性。</li><li>数据事先排序并且以 block 形式存储，有利于并行计算。</li><li>基于分布式通信框架 rabit，可以运行在 MPI 和 yarn 上。（最新已经不基于 rabit 了）</li><li>实现做了面向体系结构的优化，针对 cache 和内存做了性能优化。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;reference&quot;&gt;Reference：&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;https://www.cnblogs.com/ModifyRong/p/7744987.html&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;residual
      
    
    </summary>
    
    
      <category term="ml" scheme="http://www.yifanguo.top/tags/ml/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://www.yifanguo.top/2018/08/22/Machine%20learning-%20crash%20course/"/>
    <id>http://www.yifanguo.top/2018/08/22/Machine learning- crash course/</id>
    <published>2018-08-22T10:32:08.000Z</published>
    <updated>2018-08-22T10:32:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="machine-learning-crash-course">Machine learning- crash course</span></h1><h2><span id="framing">Framing</span></h2><h3><span id="label">Label</span></h3><p>A label is the thing we're predicting—the y variable in simple linear regression</p><ul><li>label is the true thing we are predicting: y<ul><li>the y variable in basic linear regression</li></ul></li><li>Labeled example has {features, label} (x,y)<ul><li>used to train the model</li></ul></li><li>unlabeled example has {features, ?} :(x,?)<ul><li>used for making predictions on new data</li></ul></li><li>model maps examples to predicted labels&quot; y'<ul><li>defined by internal parameters, which are learned</li></ul></li></ul><h3><span id="supervised-machine-learning">(supervised) machine learning</span></h3><p>ML systems learn how to combine input to produce useful predictions on never-before-seen data.</p><h3><span id="models">Models</span></h3><p>Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.</p><p>Inference means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference, you can predict medianHouseValue for new unlabeled examples.</p><h3><span id="regression-vs-classification">Regression vs. classification</span></h3><p>A regression model predicts continuous values. For example, regression models make predictions that answer questions like the following:</p><p>What is the value of a house in California?</p><p>What is the probability that a user will click on this ad?</p><p>A classification model predicts discrete values. For example, classification models make predictions that answer questions like the following:</p><p>Is a given email message spam or not spam?</p><p>Is this an image of a dog, a cat, or a hamster?</p><h3><span id="feature">feature</span></h3><p>good fetures are concrete and quantifiable</p><h2><span id="empirical-risk-minimization-erm">empirical risk minimization ERM</span></h2><p>Loss is the penalty for a bad prediction</p><h2><span id="squared-loss">squared loss</span></h2><p>squared loss (also known as L2 loss)</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">= the square of the difference between the label and the prediction</span><br><span class="line">  = (observation - prediction(x))2</span><br><span class="line">  = (y - y&apos;)2</span><br></pre></td></tr></table></figure></p><h2><span id="mean-square-error-mse">Mean square error MSE</span></h2><p>Mean square error (MSE) is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples:</p><h1><span id="reducing-loss">Reducing loss</span></h1><h2><span id="coverged-收敛">coverged 收敛</span></h2><p>Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has converged.</p><h3><span id="convex-凸面的">convex 凸面的</span></h3><p>Regression problems yield convex loss vs weight plots</p><p>Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.</p><h2><span id="gradient-descent">gradient descent</span></h2><p>the gradient is a vector of partial derivatives with respect to the weights</p><p>In machine learning, gradients are used in gradient descent. We often have a loss function of many variables that we are trying to minimize, and we try to do this by following the negative of the gradient of the function.</p><h2><span id="learning-rate">Learning rate</span></h2><p>As noted, the gradient vector has both a direction and a magnitude(大小）</p><p>Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called step size) to determine the next point. For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.</p><h2><span id="batch">batch</span></h2><p>In gradient descent, a batch is the total number of examples you use to calculate the gradient in a single iteration.</p><h2><span id="sgd-stochastic-gradient-descent-随机梯度下降">SGD  Stochastic Gradient Descent 随机梯度下降</span></h2><p>Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term &quot;stochastic&quot; indicates that the one example comprising each batch is chosen at random.</p><h2><span id="mini-batch-stochastic-gradient-descent-mini-batch-sgd">Mini-batch stochastic gradient descent (mini-batch SGD)</span></h2><p>is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.</p><p>#PandaDataFrame, which you can imagine as a relational data table, with rows and named columns.Series, which is a single column. A DataFrame contains one or more Series and a name for each Series.</p><p>#Generalization 泛化</p><h2><span id="overfitting-过拟合">overfitting  过拟合</span></h2><p>An overfit model gets a low loss during training but does a poor job predicting new data</p><p>The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.</p><p>Empirically 经验化</p><h2><span id="splitting-data-unexpected-high-accurate-rate">splitting data -- unexpected high accurate rate</span></h2><p>For example, consider a model that predicts whether an email is spam, using the subject line, email body, and sender's email address as features. We apportion the data into training and test sets, with an 80-20 split. After training, the model achieves 99% precision on both the training set and the test set. We'd expect a lower precision on the test set, so we take another look at the data and discover that many of the examples in the test set are duplicates of examples in the training set (we neglected to scrub duplicate entries for the same spam email from our input database before splitting the data). We've inadvertently trained on some of our test data, and as a result, we're no longer accurately measuring how well our model generalizes to new data.</p><h2><span id="overfitting">overfitting</span></h2><p>Yes indeed! The more often we evaluate on a given test set, the more we are at risk for implicitly overfitting to that one test set. We'll look at a better protocol next.</p><h2><span id="train-test">Train-Test</span></h2><p>Training set, validation set, test setif teset set metric is pool, a good signal of overfitting the validation set.</p><h1><span id="feature-enginerring">Feature Enginerring</span></h1><p>That is, one way developers hone a model is by adding and improving its features.</p><p>Feature engineering means transforming raw data into a feature vector. Expect to spend significant time doing feature engineering.</p><h2><span id="properties-of-good-feature">properties of good feature</span></h2><h2><span id="sparse-representation-稀疏表示">Sparse Representation 稀疏表示</span></h2><p>A representation of a tensor that only stores nonzero elements.</p><p>Feature sparsity refers to the sparsity of a feature vector; model sparsity refers to the sparsity of the model weights.</p><h1><span id="feature-crosses-特征交叉">Feature Crosses 特征交叉</span></h1><h1><span id="regularization-正则化-解决overfitting">Regularization 正则化 解决overfitting</span></h1><p>two ways:-- early stopping-- Penalizing the model complexity</p><p>how to define complexityprefer smaller weights</p><p>we'll now minimize loss+complexity, which is called structural risk minimization:</p><h2><span id="l2-regularization">L2 Regularization</span></h2><p>We can quantify complexity using the L2 regularization formula, which defines the regularization term as the sum of the squares of all the feature weight</p><h2><span id="regularization-rate-lamda">regularization rate -- lamda</span></h2><p>If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data.</p><h1><span id="logistic-regression">Logistic Regression</span></h1><p>calibrated 校正</p><p>regularization is super important for logistic regression</p><p>Two strategies are especially useful:L2 regularization (aka L2 weight decay) - penalizes huge weights.Early stopping - limiting training steps or learning rate.</p><h1><span id="classification">Classification</span></h1><p>Sometimes, we use logistic regression for the probability outputs -- this is a regression in (0, 1)Other times, we'll threshold the value for a discrete binary classificationChoice of threshold is an important choice, and can be tuned</p><ul><li><p>PrecissionWhat proportion of positive identifications was actually correct?</p></li><li><p>RecallWhat proportion of actual positives was identified correctly?</p></li></ul><h2><span id="roc-curve-receiver-operating-characteristic-curve">ROC curve receiver operating characteristic curve</span></h2><p>is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:</p><h2><span id="auc-area-under-the-roc-curve">AUC area under the roc curve</span></h2><p>predication bias</p><p>Don't fix bias with a calibration layer, fix it in the model.</p><h2><span id="classification-threshold-decision-threshold">classification threshold -- decision threshold</span></h2><p>in order to map a logistic regression value to a binary category, you must define a classification threshold (also called the decision threshold). A value above that threshold indicates &quot;spam&quot;; a value below indicates &quot;not spam.&quot;</p><h1><span id="regularization-sparsity">Regularization: Sparsity</span></h1><p>Caveat: Sparse feature crosses may significantly increase feature spacePossible issues:Model size (RAM) may become huge&quot;Noise&quot; coefficients (causes overfitting)</p><p>Sparse vectors often contain many dimensions. Creating a feature cross results in even more dimensions. Given such high-dimensional feature vectors, model size may become huge and require huge amounts of RAM.</p><h2><span id="l1-regularization">L1 Regularization</span></h2><p>L1 vs L2 regularization.L2 and L1 penalize weights differently:</p><p>L2 penalizes weight^2.L1 penalizes |weight|.Consequently, L2 and L1 have different derivatives:</p><p>The derivative of L2 is 2 * weight.The derivative of L1 is k (a constant, whose value is independent of weight).</p><h1><span id="neural-nets-神经网络">Neural Nets 神经网络</span></h1><h2><span id="activation-function">activation function</span></h2><p>To model a nonlinear problem, we can directly introduce a nonlinearity. We can pipe each hidden layer node through a nonlinear function.</p><p>In the model represented by the following graph, the value of each node in Hidden Layer 1 is transformed by a nonlinear function before being passed on to the weighted sums of the next layer. This nonlinear function is called the activation function.</p><h2><span id="back-propgation">back propgation</span></h2><h1><span id="multi-class-neural-nets-多类别神经网络">Multi-class Neural Nets 多类别神经网络</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;machine-learning-crash-course&quot;&gt;Machine learning- crash course&lt;/span&gt;&lt;/h1&gt;
&lt;h2&gt;&lt;span id=&quot;framing&quot;&gt;Framing&lt;/span&gt;&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python_tutorial</title>
    <link href="http://www.yifanguo.top/2018/08/13/python-tutorial/"/>
    <id>http://www.yifanguo.top/2018/08/13/python-tutorial/</id>
    <published>2018-08-12T21:08:05.000Z</published>
    <updated>2018-08-13T12:08:23.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="python" scheme="http://www.yifanguo.top/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>java_concurrent_02</title>
    <link href="http://www.yifanguo.top/2018/08/01/java-concurrent-02/"/>
    <id>http://www.yifanguo.top/2018/08/01/java-concurrent-02/</id>
    <published>2018-08-01T10:53:47.000Z</published>
    <updated>2018-08-02T03:17:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="java-concurrent-api-2">JAVA concurrent API 2</span></h1><h1><span id="futuretask">FutureTask</span></h1><p>FutureTask implements RunnableFutureRunnableFuture --&gt; Future and Runnable</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123;</span><br><span class="line">    /**</span><br><span class="line">     * Sets this Future to the result of its computation</span><br><span class="line">     * unless it has been cancelled.</span><br><span class="line">     */</span><br><span class="line">    void run();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>功能： 将计算结果从执行计算的线程传递到获取这个结果的线程，而futuretask的规范确保了这种传递过程能实现结果的安全发布</p><p>使用futuretask来提前加载稍后需要的数据</p><h1><span id="callable">Callable</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * A task that returns a result and may throw an exception.</span><br><span class="line"> * Implementors define a single method with no arguments called</span><br><span class="line"> * &#123;@code call&#125;.</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;The &#123;@code Callable&#125; interface is similar to &#123;@link</span><br><span class="line"> * java.lang.Runnable&#125;, in that both are designed for classes whose</span><br><span class="line"> * instances are potentially executed by another thread.  A</span><br><span class="line"> * &#123;@code Runnable&#125;, however, does not return a result and cannot</span><br><span class="line"> * throw a checked exception.</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;The &#123;@link Executors&#125; class contains utility methods to</span><br><span class="line"> * convert from other common forms to &#123;@code Callable&#125; classes.</span><br><span class="line"> *</span><br><span class="line"> * @see Executor</span><br><span class="line"> * @since 1.5</span><br><span class="line"> * @author Doug Lea</span><br><span class="line"> * @param &lt;V&gt; the result type of method &#123;@code call&#125;</span><br><span class="line"> */</span><br><span class="line">@FunctionalInterface</span><br><span class="line">public interface Callable&lt;V&gt; &#123;</span><br><span class="line">    /**</span><br><span class="line">     * Computes a result, or throws an exception if unable to do so.</span><br><span class="line">     *</span><br><span class="line">     * @return computed result</span><br><span class="line">     * @throws Exception if unable to compute a result</span><br><span class="line">     */</span><br><span class="line">    V call() throws Exception;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1><span id="future">Future</span></h1><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">* A &#123;@code Future&#125; represents the result of an asynchronous</span><br><span class="line">* computation.  Methods are provided to check if the computation is</span><br><span class="line">* complete, to wait for its completion, and to retrieve the result of</span><br><span class="line">* the computation.  The result can only be retrieved using method</span><br><span class="line">* &#123;@code get&#125; when the computation has completed, blocking if</span><br><span class="line">* necessary until it is ready.  Cancellation is performed by the</span><br><span class="line">* &#123;@code cancel&#125; method.  Additional methods are provided to</span><br><span class="line">* determine if the task completed normally or was cancelled. Once a</span><br><span class="line">* computation has completed, the computation cannot be cancelled.</span><br><span class="line">* If you would like to use a &#123;@code Future&#125; for the sake</span><br><span class="line">* of cancellability but not provide a usable result, you can</span><br><span class="line">* declare types of the form &#123;@code Future&lt;?&gt;&#125; and</span><br><span class="line">* return &#123;@code null&#125; as a result of the underlying task.</span><br></pre></td></tr></table></figure></p><p>good example usage<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">* interface ArchiveSearcher &#123; String search(String target); &#125;</span><br><span class="line">* class App &#123;</span><br><span class="line">*   ExecutorService executor = ...</span><br><span class="line">*   ArchiveSearcher searcher = ...</span><br><span class="line">*   void showSearch(final String target)</span><br><span class="line">*       throws InterruptedException &#123;</span><br><span class="line">*     Future&lt;String&gt; future</span><br><span class="line">*       = executor.submit(new Callable&lt;String&gt;() &#123;</span><br><span class="line">*         public String call() &#123;</span><br><span class="line">*             return searcher.search(target);</span><br><span class="line">*         &#125;&#125;);</span><br><span class="line">*     displayOtherThings(); // do other things while searching</span><br><span class="line">*     try &#123;</span><br><span class="line">*       displayText(future.get()); // use future</span><br><span class="line">*     &#125; catch (ExecutionException ex) &#123; cleanup(); return; &#125;</span><br><span class="line">*   &#125;</span><br><span class="line">* &#125;&#125;&lt;/pre&gt;</span><br></pre></td></tr></table></figure></p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">The &#123;@link FutureTask&#125; class is an implementation of &#123;@code Future&#125; that</span><br><span class="line"> * implements &#123;@code Runnable&#125;, and so may be executed by an &#123;@code Executor&#125;.</span><br><span class="line"> * For example, the above construction with &#123;@code submit&#125; could be replaced by:</span><br><span class="line"> *  &lt;pre&gt; &#123;@code</span><br><span class="line"> * FutureTask&lt;String&gt; future =</span><br><span class="line"> *   new FutureTask&lt;String&gt;(new Callable&lt;String&gt;() &#123;</span><br><span class="line"> *     public String call() &#123;</span><br><span class="line"> *       return searcher.search(target);</span><br><span class="line"> *   &#125;&#125;);</span><br><span class="line"> * executor.execute(future);&#125;&lt;/pre&gt;</span><br><span class="line"> *</span><br><span class="line"> * &lt;p&gt;Memory consistency effects: Actions taken by the asynchronous computation</span><br><span class="line"> * &lt;a href=&quot;package-summary.html#MemoryVisibility&quot;&gt; &lt;i&gt;happen-before&lt;/i&gt;&lt;/a&gt;</span><br><span class="line"> * actions following the corresponding &#123;@code Future.get()&#125; in another thread.</span><br></pre></td></tr></table></figure></p><h1><span id="httpswwwjournaldevcom1016java-thread-example">https://www.journaldev.com/1016/java-thread-example</span></h1><p>Multithreading refers to two or more threads executing concurrently in a single program. A computer single core processor can execute only one thread at a time and time slicing is the OS feature to share processor time between different processes and threads.</p><p>Runnable vs Threadextend Thread if u just want to run itimplements Runnable if u want to add more funtionality</p><p>start vs runThread.start() is required to actually create a new thread so that the runnable's run method is executed in parallel. The difference is that Thread.start() starts a thread that calls the run() method, while Runnable.run() just calls the run() method on the current thread.</p><p><strong>Thread implements Runnable</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;java-concurrent-api-2&quot;&gt;JAVA concurrent API 2&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;futuretask&quot;&gt;FutureTask&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;Futu
      
    
    </summary>
    
    
      <category term="concurrency" scheme="http://www.yifanguo.top/tags/concurrency/"/>
    
  </entry>
  
  <entry>
    <title>aqs</title>
    <link href="http://www.yifanguo.top/2018/07/31/aqs/"/>
    <id>http://www.yifanguo.top/2018/07/31/aqs/</id>
    <published>2018-07-31T13:43:17.000Z</published>
    <updated>2018-08-01T06:31:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="aqs-abstractqueuedsynchronizer-队列同步器">AQS  AbstractQueuedSynchronizer 队列同步器</span></h1><p>AQS通过内置的FIFO同步队列来完成资源获取线程的排队工作，如果当前线程获取同步状态失败（锁）时，AQS则会将当前线程以及等待状态等信息构造成一个节点（Node）并将其加入同步队列，同时会阻塞当前线程，当同步状态释放时，则会把节点中的线程唤醒，使其再次尝试获取同步状态。</p><p>AQS主要提供了如下一些方法：</p><p>getState()：返回同步状态的当前值；setState(int newState)：设置当前同步状态；compareAndSetState(int expect, int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性；tryAcquire(int arg)：独占式获取同步状态，获取同步状态成功后，其他线程需要等待该线程释放同步状态才能获取同步状态；tryRelease(int arg)：独占式释放同步状态；tryAcquireShared(int arg)：共享式获取同步状态，返回值大于等于0则表示获取成功，否则获取失败；tryReleaseShared(int arg)：共享式释放同步状态；isHeldExclusively()：当前同步器是否在独占式模式下被线程占用，一般该方法表示是否被当前线程所独占；acquire(int arg)：独占式获取同步状态，如果当前线程获取同步状态成功，则由该方法返回，否则，将会进入同步队列等待，该方法将会调用可重写的tryAcquire(int arg)方法；acquireInterruptibly(int arg)：与acquire(int arg)相同，但是该方法响应中断，当前线程为获取到同步状态而进入到同步队列中，如果当前线程被中断，则该方法会抛出InterruptedException异常并返回；tryAcquireNanos(int arg,long nanos)：超时获取同步状态，如果当前线程在nanos时间内没有获取到同步状态，那么将会返回false，已经获取则返回true；acquireShared(int arg)：共享式获取同步状态，如果当前线程未获取到同步状态，将会进入同步队列等待，与独占式的主要区别是在同一时刻可以有多个线程获取到同步状态；acquireSharedInterruptibly(int arg)：共享式获取同步状态，响应中断；tryAcquireSharedNanos(int arg, long nanosTimeout)：共享式获取同步状态，增加超时限制；release(int arg)：独占式释放同步状态，该方法会在释放同步状态之后，将同步队列中第一个节点包含的线程唤醒；releaseShared(int arg)：共享式释放同步状态；</p><h1><span id="clh-同步队列">CLH 同步队列</span></h1><p>CLH同步队列是一个FIFO双向队列，AQS依赖它来完成同步状态的管理，当前线程如果获取同步状态失败时，AQS则会将当前线程已经等待状态等信息构造成一个节点（Node）并将其加入到CLH同步队列，同时会阻塞当前线程，当同步状态释放时，会把首节点唤醒（公平锁），使其再次尝试获取同步状态。</p><h1><span id="locksupport">LockSupport</span></h1><p>当需要阻塞或者唤起一个线程时 aqs都是调用lockSupport来执行底层是通过unsafe来实现的</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">public native void park(boolean var1, long var2);</span><br><span class="line">public native void unpark(Object var1);</span><br></pre></td></tr></table></figure></p><p>unpark函数为线程提供“许可(permit)”，线程调用park函数则等待“许可”。这个有点像信号量，但是这个“许可”是不能叠加的，“许可”是一次性的。</p><p>比如线程B连续调用了三次unpark函数，当线程A调用park函数就使用掉这个“许可”，如果线程A再次调用park，则进入等待状态。</p><p>注意，unpark函数可以先于park调用。比如线程B调用unpark函数，给线程A发了一个“许可”，那么当线程A调用park时，它发现已经有“许可”了，那么它会马上再继续运行。</p><p>实际上，park函数即使没有“许可”，有时也会无理由地返回，这点等下再解析。</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Parker : public os::PlatformParker &#123;</span><br><span class="line">private:</span><br><span class="line">  volatile int _counter ;</span><br><span class="line">  ...</span><br><span class="line">public:</span><br><span class="line">  void park(bool isAbsolute, jlong time);</span><br><span class="line">  void unpark();</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br><span class="line">class PlatformParker : public CHeapObj&lt;mtInternal&gt; &#123;</span><br><span class="line">  protected:</span><br><span class="line">    pthread_mutex_t _mutex [1] ;</span><br><span class="line">    pthread_cond_t  _cond  [1] ;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到Parker类实际上用Posix的mutex，condition来实现的。在Parker类里的_counter字段，就是用来记录所谓的“许可”的。</p><p>当调用park时，先尝试直接能否直接拿到“许可”，即_counter&gt;0时，如果成功，则把_counter设置为0,并返回：</p><h1><span id="parker类实际上用posix的mutexcondition来实现的">Parker类实际上用Posix的mutex，condition来实现的</span></h1><h1><span id="posix">Posix</span></h1><p>POSIX表示可移植操作系统接口（Portable Operating System Interface of UNIX，缩写为 POSIX ）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;aqs-abstractqueuedsynchronizer-队列同步器&quot;&gt;AQS  AbstractQueuedSynchronizer 队列同步器&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;AQS通过内置的FIFO同步队列来完成资源获取线程的排队工作，如果当前
      
    
    </summary>
    
    
      <category term="concurrency" scheme="http://www.yifanguo.top/tags/concurrency/"/>
    
  </entry>
  
  <entry>
    <title>Atomic</title>
    <link href="http://www.yifanguo.top/2018/07/30/Atomic/"/>
    <id>http://www.yifanguo.top/2018/07/30/Atomic/</id>
    <published>2018-07-30T14:04:50.000Z</published>
    <updated>2018-07-31T05:11:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="atomicity-optimizations">Atomicity Optimizations</span></h1><p>If multiple threads modify the same memory location concurrently, processors do not guarantee any specific result.</p><h1><span id="lock">Lock</span></h1><p>Load Lock/Store Conditional (LL/SC)41 The LL/SC operations work as a pair where the special load instruction is used to start an transaction and the final store will only succeed if the location has not been modified in the meantime. The store oper- ation indicates success or failure, so the program can repeat its efforts if necessary.</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int curval;int newval;do &#123;  curval = LL(var);  newval = curval + addend;&#125; while (SC(var, newval));</span><br></pre></td></tr></table></figure></p><p>#CAS</p><p>Compare-and-Swap (CAS) This is a ternary operation which writes a value provided as a parameter into an address (the second parameter) only if the cur- rent value is the same as the third parameter value;</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">int curval;</span><br><span class="line">int newval;</span><br><span class="line">do &#123;</span><br><span class="line">curval = var;</span><br><span class="line">  newval = curval + addend;</span><br><span class="line">&#125; while (CAS(&amp;var, curval, newval));</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;atomicity-optimizations&quot;&gt;Atomicity Optimizations&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;If multiple threads modify the same memory location concurrent
      
    
    </summary>
    
    
      <category term="concurrency" scheme="http://www.yifanguo.top/tags/concurrency/"/>
    
  </entry>
  
  <entry>
    <title>WhatEveryProgrammerShouldKnowAboutMemory</title>
    <link href="http://www.yifanguo.top/2018/07/30/WhatEveryProgrammerShouldKnowAboutMemory/"/>
    <id>http://www.yifanguo.top/2018/07/30/WhatEveryProgrammerShouldKnowAboutMemory/</id>
    <published>2018-07-29T21:25:51.000Z</published>
    <updated>2018-07-31T04:50:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="超线程">超线程</span></h1><p>refer: https://www.cnblogs.com/Amaranthus/archive/2013/07/09/3180036.htmlHyper-threading enables a single processor core to be used for two or more concurrent executions with just a little extra hardware.</p><h1><span id="现代计算机的架构">现代计算机的架构</span></h1><p>主要有两个部分组成，南北桥<img src="nsbridge.png" alt=""></p><p>All CPUs (two in the previous example, but there can be more) are connected via a common bus (the Front Side Bus, FSB) to the Northbridge.</p><p>所有的cpu都通过fsb连接到北桥 <strong>fsb已经被qpi淘汰</strong></p><h2><span id="北桥">北桥</span></h2><p>The Northbridge contains, among other things, the memory controller, and its im- plementation determines the type of RAM chips used for the computer. Different types of RAM, such as DRAM, Rambus, and SDRAM, require different memory con- trollers.</p><p>北桥包含了内存控制器，决定了RAM(random access memory)的类型</p><p>To reach all other system devices, the Northbridge must communicate with the Southbridge.为了接触到其他的devices, 北桥必须和南桥通信</p><h2><span id="南桥">南桥</span></h2><p>南桥又被称为  I/O bridge 通过一系列的bus和其他devices通信</p><p>the PCI, PCI Express, SATA, and USB buses are of most importance, but PATA, IEEE 1394, serial, and par- allel ports are also supported by the Southbridge.</p><p>However, today the PCI-E slots are all connected to the Southbridge.</p><p>这样的结构造成了以下的结果</p><p>##所有的data communication 从一个cpu到另一个cpu必须 travel over the same bus used to communicate with the Northbridge.<strong>这个已经有所改变，intel最新的多核处理器是通过qpi进行通信的</strong>qpi: quick path iterconnecthttps://www.intel.com/content/www/us/en/io/quickpath-technology/quick-path-interconnect-introduction-paper.html</p><p>The Intel QuickPath Interconnect (QPI) is a point-to-point processor interconnect developed by Intel which replaced the front-side bus (FSB) in Xeon, Itanium, and certain desktop platforms starting in 2008.  Prior to the name's announcement, Intel referred to it as Common System Interface (CSI).</p><p>... 2008年推出的技术，结果这篇文章是2007年的</p><p>QPI will be replaced by Intel UltraPath Interconnect (UPI) in future Skylake EX/EP Xeon processors based on LGA 3647 socket.[5]</p><p>QPI也要被淘汰...</p><p>the QuickPath Architecture assumes that the processors will have integrated memory controllers, and enables a non-uniform memory access (NUMA) architecture.</p><h2><span id="all-communication-with-ram-must-pass-through-the-northbridge">All communication with RAM must pass through the Northbridge.</span></h2><h2><span id="the-ram-has-only-a-single-port">The RAM has only a single port</span></h2><h2><span id="communication-between-a-cpu-and-a-device-at-tached-to-the-southbridge-is-routed-through-the-northbridge">Communication between a CPU and a device at- tached to the Southbridge is routed through the Northbridge.</span></h2><h1><span id="dma-direct-memory-access">DMA direct memory access</span></h1><p>DMA allows devices, with the help of the Northbridge, to store and receive data in RAM directly without the intervention of the CPU (and its inherent performance cost).</p><p>it also creates contention for the bandwidth of the Northbridge as DMA requests compete with RAM access from the CPUs.</p><ol><li>DMA允许设备直接和RAM交互</li><li>DMA会和CPU抢夺北桥带宽</li></ol><h1><span id="第二个bottleneck">第二个bottleneck</span></h1><p>recent RAM types require two sep- arate buses (or channels as they are called for DDR2, see page 8) which doubles the available bandwidth.</p><p>北桥和RAM之间的总线所以出现了双通道（可以实现带宽加倍，内存访问在两个通道上交错分配）</p><p>北桥自身不带内存控制器，而是连接到外部多个内存控制器上，好处是支持更多的内存，可以同时访问不同的内存区，降低了延迟，但是对北桥的内部带宽要求巨大。使用外部内存控制器并不是唯一的办法，比较流行的还有一种是把控制器集成到cpu内部，将内存直接连接到CPU<img src="nbridge.png" alt="">The advantage of this architecture is that more than one memory bus exists and therefore total available band- width increases</p><p>这样的架构，系统里有几个cpu就可以有几个内存库（memory bank），不需要强大的北桥就可以实现4倍的内存带宽。但是缺点也是很明显：1.导致内存不再是统一的资源（NUMA的得名），2.cpu可以正常的访问本地内存，但是访问其他内存时需要和其他cpu互通。在讨论访问远端内存的代价时，我们用「NUMA因子」这个词。比如说IBM的x445和SGI的Altix系列。CPU被归入节点，节点内的内存访问时间是一致的，或者只有很小的NUMA因子。而在节点之间的连接代价很大，而且有巨大的NUMA因子。</p><p>#CSIIntel will have support for the Common System Interface (CSI) starting with the Nehalem processors; this is basically the same approach: an integrated memory controller with the possibility of local memory for each processor.</p><p>cpu集成RAM</p><p><img src="imc.png" alt=""></p><p>优点：不需要北桥庞大的带宽缺点：memory is not uniform ( numa- non uniform memroy architecture)</p><h1><span id="ram-types">RAM types</span></h1><p>RAM主要分为2类静态RAM，动态RAM，前者速度快，代价搞，后者速度慢代价低</p><p>SRAM 比DRAM 更贵</p><h1><span id="static-ram">static RAM</span></h1><p><img src="SRAM.png" alt=""></p><p>6个二极管组成的static dram</p><p>They have two stable states, representing 0 and 1 respectively. The state is stable as long as power on Vdd is available.</p><p><strong>这张图就充分说明了为什么断电RAM中的内容会消失</strong></p><p>主要有6个晶体管组成，核心是4个晶体管M1-M4,他们有2个稳定状态分别代表0和1</p><p>If access to the state of the cell is needed the word access line WL is raised. This makes the state of the cell imme- diately available for reading on BL and BL. If the cell state must be overwritten the BL and BL lines are first set to the desired values and then WL is raised. Since the outside drivers are stronger than the four transistors (M1 through M4) this allows the old state to be overwritten.</p><h2><span id="sram的特点">SRAM的特点</span></h2><p>• one cell requires six transistors. There are variants with four transistors but they have disadvantages.• maintaining the state of the cell requires constant power.• the cell state is available for reading almost im- mediately once the word access line WL is raised. The signal is as rectangular (changing quickly be- tween the two binary states) as other transistor- controlled signals.• the cell state is stable, no refresh cycles are needed.</p><h1><span id="dynamic-ram">Dynamic RAM</span></h1><p><img src="dram.png" alt=""></p><p>All it consists of is one transistor and one capacitor动态RAM只有一个晶体管和一个电容</p><ol><li>A dynamic RAM cell keeps its state in the capacitor C.</li><li>The transistor M is used to guard the access to the state.</li><li>To read the state of the cell the access line AL is raised;</li><li>this either causes a current to flow on the data line DL or not, depending on the charge in the capacitor.</li><li>To write to the cell the data line DL is appropriately set and then AL is raised for a time long enough to charge or drain the capacitor.</li></ol><p>disadvantages:</p><h2><span id="readiing-the-cell-discharges-the-capacitor">readiing the cell discharges the capacitor</span></h2><p>动态RAM优点是简单，但是缺点是由于读取状态时需要对电容器放电，所以这一过程不能无限重复，不得不在某个点上对它重新充电。更糟糕的是，为了容纳大量单元(现在一般在单个芯片上容纳10的9次方以上的RAM单元)，电容器的容量必须很小(0.000000000000001法拉以下)。这样，完整充电后大约持有几万个电子。即使电容器的电阻很大(若干兆欧姆)，仍然只需很短的时间就会耗光电荷，称为「泄漏」。</p><p><strong>这种泄露就是现在的大部分DRAM芯片每隔64ms就必须进行一次刷新的原因。（附A关于三极管的输入输出特性</strong></p><p>During the refresh cycle no access to the memory is possible since a refresh is simply a memory read operation where the result is discarded.</p><p>For some workloads this overhead might stall up to 50% of the memory accesses</p><h2><span id="a-second-problem-resulting-from-the-tiny-charge-is-that-the-information-read-from-the-cell-is-not-directly-usable">A second problem resulting from the tiny charge is that the information read from the cell is not directly usable.</span></h2><p>The data line must be connected to a sense amplifier which can distinguish between a stored 0 or 1 over the whole range of charges which still have to count as 1.</p><h2><span id="a-third-problem-is-that-reading-a-cell-causes-the-charge-of-the-capacitor-to-be-depleted">A third problem is that reading a cell causes the charge of the capacitor to be depleted.</span></h2><p>This means every read operation must be followed by an operation to recharge the capacitor.</p><h2><span id="高能预警why-sram-is-faster-than-dram">高能预警，why sram is faster than dram</span></h2><p>Unlike the static RAM case where the output is immediately available when the word access line is raised, it will always take a bit of time until the capacitor discharges sufficiently. This delay severely limits how fast DRAM can be.</p><p>因为DRAM把01状态保存在电容里，导致每次读都需要电容放电</p><h2><span id="dram-优势-size">DRAM 优势 --size</span></h2><p>The SRAM cells also need individual power for the transistors maintaining the state.</p><h1><span id="dram-access">DRAM access</span></h1><p>A program selects a memory location using a virtual address</p><p>The processor translates this into a physical address and finally the memory controller selects the RAM chip corresponding to that address.</p><p>Dynamic RAM Schematic</p><p><img src="data.png" alt=""></p><p>de-multiplexier<img src="demultiplexer" alt=""></p><p>The memory controller must be able to address each RAM module (collection of RAM chips).</p><h1><span id="sram-vs-dram">SRAM vs DRAM</span></h1><p>SRAM is currently used in CPU caches and on-die where the connections are small and fully under control of the CPU designer.</p><h1><span id="dram-access-technical-details">DRAM Access Technical Details</span></h1><ol><li>In the section introducing DRAM we saw that DRAM chips multiplex the addresses in order to save resources int the form of address pins.</li><li>We also saw that access- ing DRAM cells takes time since the capacitors in those cells do not discharge instantaneously to produce a stable signal</li><li>we also saw that DRAM cells must be refreshed.</li></ol><h1><span id="dram-sdram-and-its-successors-double-data-rate-dram-ddr">DRAM (SDRAM) and its successors Double Data Rate DRAM (DDR).</span></h1><h1><span id="sdram-synchronous-dram">SDRAM - Synchronous DRAM</span></h1><p>同步DRAM，顾名思义，是参照一个时间源工作的。由内存控制器提供一个时钟，时钟的频率决定了前端总线(FSB)的速度。以今天的SDRAM为例，每次数据传输包含64位，即8字节。所以FSB的传输速率应该是有效总线频率乘于8字节(对于4倍传输200MHz总线而言，传输速率为6.4GB/s)。听起来很高，但要知道这只是峰值速率，实际上无法达到的最高速率。我们将会看到，与RAM模块交流的协议有大量时间是处于非工作状态，不进行数据传输。我们必须对这些非工作时间有所了解，并尽量缩短它们，才能获得最佳的性能。</p><h1><span id="221读访问协议">2.2.1读访问协议</span></h1><p>这里忽略了许多细节，我们只关注时钟频率、RAS与CAS信号、地址总线和数据总线。首先，内存控制器将行地址放在地址总线上，并降低RAS信号，读周期开始。所有信号都在时钟(CLK)的上升沿读取，因此，只要信号在读取的时间点上保持稳定，就算不是标准的方波也没有关系。设置行地址会促使RAM芯片锁住指定的行。</p><p>CAS信号在tRCD(RAS到CAS时延)个时钟周期后发出。内存控制器将列地址放在地址总线上，降低CAS线。这里我们可以看到，地址的两个组成部分是怎么通过同一条总线传输的。</p><p>既然数据的传输需要这么多的准备工作，仅仅传输一个字显然是太浪费了。因此，DRAM模块允许内存控制指定本次传输多少数据。可以是2、4或8个字。这样，就可以一次填满高速缓存的整条线，而不需要额外的RAS/CAS序列。另外，内存控制器还可以在不重置行选择的前提下发送新的CAS信号。这样，读取或写入连续的地址就可以变得非常快，因为不需要发送RAS信号，也不需要把行置为非激活状态(见下文)。</p><p>在上图中，SDRAM的每个周期输出一个字的数据。这是第一代的SDRAM。而DDR可以在一个周期中输出两个字。这种做法可以减少传输时间，但无法降低时延。</p><p><img src="address.png" alt=""></p><h1><span id="222预充电和激活">2.2.2预充电和激活</span></h1><p>2.2.1中的图只是读取数据的一部分，还有以下部分：</p><p>显示的是两次CAS信号的时序图。第一次的数据在CL周期后准备就绪。图中的例子里，是在SDRAM上，用两个周期传输了两个字的数据。如果换成DDR的话，则可以传输4个字。即使是在一个命令速率为1的DRAM模块上，也无法立即发出预充电命令，而要等数据传输完成。在上图中，即为两个周期。刚好与CL相同，但只是巧合而已。预充电信号并没有专用线，某些实现是用同时降低写使能(WE)线和RAS线的方式来触发。</p><p>发出预充电信命令后，还需等待tRP(行预充电时间)个周期之后才能使行被选中。在图2.9中，这个时间(紫色部分)大部分与内存传输的时间(淡蓝色部分)重合。不错。但tRP大于传输时间，因此下一个RAS信号只能等待一个周期。</p><p>数据总线的7个周期中只有2个周期才是真正在用的。再用它乘于FSB速度，结果就是，800MHz总线的理论速率6.4GB/s降到了1.8GB/s</p><p>我们会看到预充电指令被数据传输时间限制（途中为COL Addr的传输）除此之外，SDRAM模块在RAS信号之后，需要经过一段时间，才能进行预充电(记为tRAS)（minimum active to precharge time（也就是RAS信号之后到充电的最小时间间隔））它的值很大，一般达到tRP的2到3倍。如果在某个RAS信号之后，只有一个CAS信号，而且数据只传输很少几个周期，那么就有问题了。假设在图2.9中，第一个CAS信号是直接跟在一个RAS信号后免的，而tRAS为8个周期。那么预充电命令还需要被推迟一个周期，因为tRCD、CL和tRP加起来才7个周期。</p><p>DDR模块往往用w-z-y-z-T来表示。例如，2-3-2-8-T1，意思是：</p><p>w 2 CAS时延(CL)x 3 RAS-to-CAS时延(t RCD)y 2 RAS预充电时间(t RP)z 8 激活到预充电时间(t RAS)T T1 命令速率</p><h1><span id="223重充电">2.2.3重充电</span></h1><p>充电对内存是性能最大的影响，根据JEDEC规范，DRAM单元必须保持每64ms刷新一次我们在解读性能参数时有必要知道，它也是DRAM生命周期的一个部分。如果系统需要读取某个重要的字，而刚好它所在的行正在刷新，那么处理器将会被延迟很长一段时间。刷新的具体耗时取决于DRAM模块本身。</p><h1><span id="225-结论">2.2.5 结论</span></h1><p>通过本节，大家应该了解到访问DRAM的过程并不是一个快速的过程。至少与处理器的速度相比，或与处理器访问寄存器及缓存的速度相比，DRAM的访问不算快。大家还需要记住CPU和内存的频率是不同的。Intel Core 2处理器运行在2.933GHz，而1.066GHz FSB有11:1的时钟比率(注: 1.066GHz的总线为四泵总线)。那么，内存总线上延迟一个周期意味着处理器延迟11个周期。绝大多数机器使用的DRAM更慢，因此延迟更大。前文中读命令的时序图表明，DRAM模块可以支持高速数据传输。每个完整行可以被毫无延迟地传输。数据总线可以100%被占。对DDR而言，意味着每个周期传输2个64位字。对于DDR2-800模块和双通道而言，意味着12.8GB/s的速率。</p><p>但是，除非是特殊设计，DRAM的访问并不总是串行的。访问不连续的内存区意味着需要预充电和RAS信号。于是，各种速度开始慢下来，DRAM模块急需帮助。预充电的时间越短，数据传输所受的惩罚越小。</p><p>硬件和软件的预取(参见第6.3节)可以在时序中制造更多的重叠区，降低延迟。预取还可以转移内存操作的时间，从而减少争用。我们常常遇到的问题是，在这一轮中生成的数据需要被存储，而下一轮的数据需要被读出来。通过转移读取的时间，读和写就不需要同时发出了</p><p>2.3主存的其他用户除了CPU外，系统中还有其它一些组件也可以访问主存。高性能网卡或大规模存储控制器是无法承受通过CPU来传输数据的，它们一般直接对内存进行读写(直接内存访问，DMA)。在图2.1中可以看到，它们可以通过南桥和北桥直接访问内存。另外，其它总线，比如USB等也需要FSB带宽，即使它们并不使用DMA，但南桥仍要通过FSB连接到北桥。</p><p>DMA当然有很大的优点，但也意味着FSB带宽会有更多的竞争。在有大量DMA流量的情况下，CPU在访问内存时必然会有更大的延迟。我们可以用一些硬件来解决这个问题。例如，通过图2.3中的架构，我们可以挑选不受DMA影响的节点，让它们的内存为我们的计算服务。还可以在每个节点上连接一个南桥，将FSB的负荷均匀地分担到每个节点上。</p><h1><span id="what-programmer-can-do">what programmer can do</span></h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;超线程&quot;&gt;超线程&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;refer: https://www.cnblogs.com/Amaranthus/archive/2013/07/09/3180036.html
Hyper-threading enables a si
      
    
    </summary>
    
    
      <category term="core" scheme="http://www.yifanguo.top/tags/core/"/>
    
  </entry>
  
  <entry>
    <title>lock</title>
    <link href="http://www.yifanguo.top/2018/07/30/lock/"/>
    <id>http://www.yifanguo.top/2018/07/30/lock/</id>
    <published>2018-07-29T20:46:26.000Z</published>
    <updated>2018-07-30T12:09:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1><span id="深入理解锁机制的实现">深入理解锁机制的实现</span></h1><h1><span id="cas">CAS</span></h1><p>Unsafe中有一个method compareAndSwapInt 实现的是无锁同步的机制我们看下它是如何实现的</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">/***</span><br><span class="line">         * Compares the value of the integer field at the specified offset</span><br><span class="line">         * in the supplied object with the given expected value, and updates</span><br><span class="line">         * it if they match.  The operation of this method should be atomic,</span><br><span class="line">         * thus providing an uninterruptible way of updating an integer field.</span><br><span class="line">         * 在obj的offset位置比较integer field和期望的值，如果相同则更新。这个方法</span><br><span class="line">         * 的操作应该是原子的，因此提供了一种不可中断的方式更新integer field。</span><br><span class="line">         *</span><br><span class="line">         * @param obj the object containing the field to modify.</span><br><span class="line">         *            包含要修改field的对象</span><br><span class="line">         * @param offset the offset of the integer field within &lt;code&gt;obj&lt;/code&gt;.</span><br><span class="line">         *               &lt;code&gt;obj&lt;/code&gt;中整型field的偏移量</span><br><span class="line">         * @param expect the expected value of the field.</span><br><span class="line">         *               希望field中存在的值</span><br><span class="line">         * @param update the new value of the field if it equals &lt;code&gt;expect&lt;/code&gt;.</span><br><span class="line">         *           如果期望值expect与field的当前值相同，设置filed的值为这个新值</span><br><span class="line">         * @return true if the field was changed.</span><br><span class="line">         *                             如果field的值被更改</span><br><span class="line">         */</span><br><span class="line">         </span><br><span class="line">public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);</span><br></pre></td></tr></table></figure></p><p>调用了JNI，也就是说有对应的unsafe.cpp的接口 如下：</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSwapInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x))</span><br><span class="line">  UnsafeWrapper(&quot;Unsafe_CompareAndSwapInt&quot;);</span><br><span class="line">  oop p = JNIHandles::resolve(obj);</span><br><span class="line">  //获取对象的变量的地址</span><br><span class="line">  jint* addr = (jint *) index_oop_from_field_offset_long(p, offset);</span><br><span class="line">  //调用Atomic操作</span><br><span class="line">  //进入atomic.hpp,大意就是先去获取一次结果，如果结果和现在不同，就直接返回，因为有其他人修改了；否则会一直尝试去修改。直到成功。</span><br><span class="line">  return (jint)(Atomic::cmpxchg(x, addr, e)) == e;</span><br><span class="line">UNSAFE_END</span><br></pre></td></tr></table></figure></p><p>再来看atomic::cmpxchg这个方法实现，这是一个c++的库</p><p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">jbyte Atomic::cmpxchg(jbyte exchange_value, volatile jbyte* dest, jbyte compare_value) &#123;</span><br><span class="line">  assert(sizeof(jbyte) == 1, &quot;assumption.&quot;);</span><br><span class="line">  uintptr_t dest_addr = (uintptr_t)dest;</span><br><span class="line">  uintptr_t offset = dest_addr % sizeof(jint);</span><br><span class="line">  volatile jint* dest_int = (volatile jint*)(dest_addr - offset);</span><br><span class="line">  jint cur = *dest_int;</span><br><span class="line">  jbyte* cur_as_bytes = (jbyte*)(&amp;cur);</span><br><span class="line">  jint new_val = cur;</span><br><span class="line">  jbyte* new_val_as_bytes = (jbyte*)(&amp;new_val);</span><br><span class="line">  new_val_as_bytes[offset] = exchange_value;</span><br><span class="line">  while (cur_as_bytes[offset] == compare_value) &#123;</span><br><span class="line">    jint res = cmpxchg(new_val, dest_int, cur);</span><br><span class="line">    if (res == cur) break;</span><br><span class="line">    cur = res;</span><br><span class="line">    new_val = cur;</span><br><span class="line">    new_val_as_bytes[offset] = exchange_value;</span><br><span class="line">  &#125;</span><br><span class="line">  return cur_as_bytes[offset];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>jint* dest_int 做了volatile，意味着值不会从cpu cache中获取，会从主内存中获取</p><p>CAS是Compare And Set的缩写。CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。</p><h1><span id="x86">x86</span></h1><p>CAS保证了原子性，原子性定义Atomic Variables. The java.util.concurrent.atomic package defines classes that support atomic operations on single variables. All classes have get and set methods that work like reads and writes on volatile variables. That is, a set has a happens-before relationship with any subsequent get on the same variable.</p><p>简单来说就是原子变量一旦开始操作就不会做context switch（切换到另一个线程），直到运行结束。</p><h2><span id="什么情况下适合用atomic类">什么情况下适合用atomic类</span></h2><p>Atomic classes are designed primarily as building blocks for implementing non-blocking data structures and related infrastructure classes. The compareAndSet method is not a general replacement for locking. It applies only when critical updates for an object are confined to a single variable.</p><p>那么在x86架构中是如何实现的</p><h1><span id="m-threads-同时-sharedcounter">M threads 同时++ sharedCounter</span></h1><p>non-volatile version:add    $0x10,%ecx</p><p>volatile version:mov    0xc(%r10),%r8d ; Loadinc    %r8d           ; Incrementmov    %r8d,0xc(%r10) ; Storelock addl $0x0,(%rsp) ; StoreLoad Barrier</p><p>makes every store before the lock addl visible to other processors, and ensures that every load after the lock addl gets at least the version visible at the time it is executed. In this case, volatile gives visibility, in that each of the processors immediately gets the version from the other processors after each increment.</p><p>AtomicInteger version:</p><p>mov    0xc(%r11),%eax       ; Loadmov    %eax,%r8d<br>inc    %r8d                 ; Incrementlock cmpxchg %r8d,0xc(%r11) ; Compare and exchange</p><p>cmpxchg:Compares the value in the EAX register with the destination operand. If the two values are equal, the source operand is loaded into the destination operand. Otherwise, the destination operand is loaded into the EAX register.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[toc]&lt;/p&gt;
&lt;h1&gt;&lt;span id=&quot;深入理解锁机制的实现&quot;&gt;深入理解锁机制的实现&lt;/span&gt;&lt;/h1&gt;
&lt;h1&gt;&lt;span id=&quot;cas&quot;&gt;CAS&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;Unsafe中有一个method compareAndSwapInt 实现的是无
      
    
    </summary>
    
    
      <category term="lock" scheme="http://www.yifanguo.top/tags/lock/"/>
    
  </entry>
  
  <entry>
    <title>unix network programming</title>
    <link href="http://www.yifanguo.top/2018/07/30/unix/"/>
    <id>http://www.yifanguo.top/2018/07/30/unix/</id>
    <published>2018-07-29T17:53:18.000Z</published>
    <updated>2018-07-30T09:07:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1><span id="unix-network-programming-learning-notes">unix network programming learning notes</span></h1><p>daemon: 后台程序</p><p>a Web server is typically thought of as a long-running program (or daemon) that sends network messages only in response to requests coming in from the network. The other side of the protocol is a Web client, such as a browser, which always initiates communication with the server.</p><p>Web clients and servers communicate using the Transmission Control Protocol, or TCP. TCP, in turn, uses the Internet Protocol, or IP, and IP communicates with a datalink layer of some form.</p><p><img src="tcp.png" alt=""></p><p>Even though the client and server communicate using an application protocol, the transport layers communicate using TCP</p><p>he client and server need not be attached to the same local area network (LAN) as we show in Figure 1.3. For instance, in Figure 1.4, we show the client and server on different LANs, with both LANs connected to a wide area network (WAN) using routers.</p><p><img src="wan.png" alt=""></p><p>The largest WAN today is the Internet.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;&lt;span id=&quot;unix-network-programming-learning-notes&quot;&gt;unix network programming learning notes&lt;/span&gt;&lt;/h1&gt;
&lt;p&gt;daemon: 后台程序&lt;/p&gt;
&lt;p&gt;a Web serv
      
    
    </summary>
    
    
      <category term="unix" scheme="http://www.yifanguo.top/tags/unix/"/>
    
  </entry>
  
</feed>
